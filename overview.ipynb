{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog here: https://wordpress.com/post/brianfarish3.wordpress.com/168\n",
    "\n",
    "We are here: https://docs.aws.amazon.com/textract/latest/dg/api-async.html\n",
    "\n",
    "Diagram of this pipeline: https://app.diagrams.net/#Hcodeslp%2Faws_arch_diagrams%2Fmaster%2Ftextract-stepfns.drawio\n",
    "\n",
    "AWS has this which could be helpful to me in writing my code in the testract JSON to RDS Lambda.\n",
    "\n",
    "boto3 textract documentation: \n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#textract\n",
    "\n",
    "AWS recommends using an asynchronous function to process multiple page files. I'm going to use step functions. More details on that decision in the blog."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IAM\n",
    "The role will be named: step-functions-lambda-role. The tag is \"app\": \"document-parser\"\n",
    "I am creating an IAM role for the Lambda function that will allow Step Functions to access resources. The policy will be called \"AllowStepFunctionStateMachineStart\". It uses the AWS standard policy AWSLambdaRole, which generates cloudwatch logs. Also I am adding permission to start state machines with this policy: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"states:StartExecution\",\n",
    "            \"Resource\": \"arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEP FUNCTION\n",
    "I'm creating a step function called Doc-Parser-Pipeline and attaching the above role to it. I'm not turning on logs now, but may later, as I inevitably mess up. Maybe even tracing to with X-ray. Tagged with app:document-parser.\n",
    "\n",
    "State machine arn:  arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMBDA\n",
    "I'm creating a step function \"run-step-function-lambda\" tagged with app:document-parser.\n",
    "\n",
    "Adding environment variable:\n",
    "STATEMACHINEARN:arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3\n",
    "Creating document upload bucket. Not public. Tag app:document-parser\n",
    "Bucket name: \n",
    "doc-upload-bucket-doc-parser-app\n",
    "S3 BUCKET ARN:\n",
    "arn:aws:s3:::doc-upload-bucket-doc-parser-app\n",
    "\n",
    "Properties > event notifications > create\n",
    "Event name: \n",
    "trigger-document-parsing-event \n",
    "\n",
    "Suffix:\n",
    ".pdf\n",
    "\n",
    "Object creation, checking \"all object create events\". From the Lambda dropdown at the bottom selecting \"run-step-functions-lambda\"\n",
    "\n",
    "Creating an S3 bucket for textract's JSON object output objects used during the pipeline. No triggers on it for now.\n",
    "\n",
    "Name: \n",
    "textract-json-output-parser-app\n",
    "\n",
    "ARN:\n",
    "arn:aws:s3:::textract-json-output-parser-app\n",
    "\n",
    "We need a bocket policy on this last bucket: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Id\": \"Policy1683945477760\",\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "      {\n",
    "        \"Sid\": \"Stmt1683945475477\",\n",
    "        \"Action\": \"s3:PutObject\",\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Resource\": \"arn:aws:s3:::textract-json-output-parser-app/*\",\n",
    "        \"Principal\": {\n",
    "          \"AWS\": [\n",
    "            \"arn:aws:iam::415832459288:role/service-role/doc-to-json-textract-role-7n40j7gg\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMBDA\n",
    "\n",
    "We are going to create three Lambdas ***BUT*** they will not actually have the logic we need in them yet. They are basically placeholders. These are in order that they are invoked in the step function:\n",
    "\n",
    "Name:\n",
    "doc-to-json-textract\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:doc-to-json-textract\n",
    "\n",
    "Name:\n",
    "textract-status-checker\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:textract-status-checker\n",
    "\n",
    "Name:\n",
    "parse-textract-json-obj-for-rds\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:parse-textract-json-obj-for-rds\n",
    "\n",
    "# Reminder: At this point, all of these (and the initial run-step-functions-lambda also) are placeholders and contain no real logic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FUNCTIONS\n",
    "\n",
    "JSON will be pasted after this block.\n",
    "\n",
    "Editing - Deleting the default Hello and World states from the \"States\" key. \n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > doc-to-json-textract.\n",
    "\n",
    "Click copy to copy the invoke JSON and paste into definition code snippet after \"States\": {. Click format JSON to clean it up.\n",
    "Rename \"Invoke function\" to \"Start extraction\".  Make sure key above is \"StartAt\": \"Start Extraction\"\n",
    "\n",
    "Change Next key to \"Next:\" \"Wait for textract\"\n",
    "\n",
    "Add in Wait State. Change name to \"Wait for Textract\"\n",
    "\n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > textract-status-checker.  Change first line of new add from Invoke function to \"Check textract status\".  Make sure above key in Wait state is \"Next\": \"Check textract status\".\n",
    "\n",
    "Add a code snippet for a Choice state. Will be edited heavily see code below. Don't need a \"NOT\" block. (The default state later will go back to the Wait state.) Essentially two blocks with \"StringEquals\": \"COMPLETED\" or second with \"StringEquals\": \"FAILED\". The variable \"$.Payload.TextractJobStatus\".\n",
    "\n",
    "Change kv Default to \"Default\": \"Wait for textract\"\n",
    "\n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > parse-textract-json-obj-for-rds.  Change first line of new add from Invoke function to \"Parse textract JSON\".  Make sure above key in Wait state is \"Next\": \"Parse textract JSON\".\n",
    "\n",
    "In choices COMPLETED block set kv \"Next\": \"Parse textract JSON\".\n",
    "\n",
    "Change last \"Next\" to kv \"End\": true\n",
    "\n",
    "For FAILED Choice block:\n",
    "Generate snippet > flow control state > Fail state. Copy. Paste after END. Change as you see. Set FAILED block kv \"Next\": \"Textract failed\"\n",
    "\n",
    "Chart here: \n",
    "/Users/bfaris96/Desktop/data_engineer/FCC-doc-tables/school_assessments_proj/Screen Shot 2023-05-08 at 3.59.30 PM.png\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"run-step-function-lambda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_function_client = boto3.client('stepfunctions')\n",
    "    state_machine_arn = os.environ['STATEMACHINEARN']\n",
    "    \n",
    "    # Extract relevant information from the S3 event\n",
    "    s3_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    s3_object_key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    # Prepare the input for the state machine\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_object_key\n",
    "    }\n",
    "    \n",
    "    # Start the execution of the state machine\n",
    "    response = step_function_client.start_execution(\n",
    "        stateMachineArn=state_machine_arn,\n",
    "        input=json.dumps(step_state)\n",
    "    )\n",
    "\n",
    "    return json.dumps(response, default=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"doc-to-json-textract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    textract_client = boto3.client('textract')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']\n",
    "    s3_bucket = step_state['s3Bucket']   # this is the initial upload bucket\n",
    "    s3_object_key = step_state['s3ObjectKey']   # this is the initial upload object\n",
    "\n",
    "    # Prepare the output bucket\n",
    "    output_bucket = \"textract-json-output-parser-app\"\n",
    "\n",
    "    # Start the Textract job for the uploaded document using StartDocumentTextDetection API for extracting raw text\n",
    "    start_document_text_detection_response = textract_client.start_document_text_detection(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3_bucket,\n",
    "                'Name': s3_object_key\n",
    "            }\n",
    "        },\n",
    "        OutputConfig={\n",
    "            'S3Bucket': output_bucket,\n",
    "            'S3Prefix': f\"{s3_object_key}-text-detection/\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Start the Textract job for the uploaded document using StartDocumentAnalysis API for extracting tables\n",
    "    start_document_analysis_response = textract_client.start_document_analysis(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3_bucket,\n",
    "                'Name': s3_object_key\n",
    "            }\n",
    "        },\n",
    "        FeatureTypes=['TABLES'],\n",
    "        OutputConfig={\n",
    "            'S3Bucket': output_bucket,\n",
    "            'S3Prefix': f\"{s3_object_key}-table-analysis/\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Creating keys for the text detection and table analysis output objects\n",
    "    text_detection_output_prefix = f\"{s3_object_key}-text-detection/\"\n",
    "    table_analysis_output_prefix = f\"{s3_object_key}-table-analysis/\"\n",
    "    step_state['textDetectionOutputObjectKey'] = f\"{text_detection_output_prefix}{s3_object_key}.json\"\n",
    "    step_state['tableAnalysisOutputObjectKey'] = f\"{table_analysis_output_prefix}{s3_object_key}.json\"\n",
    "\n",
    "    # Update the step-state with the Textract JobIds\n",
    "    step_state['textDetectionJobId'] = start_document_text_detection_response['JobId']\n",
    "    step_state['tableAnalysisJobId'] = start_document_analysis_response['JobId']\n",
    "\n",
    "    return step_state\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"textract-status-checker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    textract_client = boto3.client('textract')\n",
    "\n",
    "    # Retrieve the Textract JobIds, S3 bucket, and object key from the input event\n",
    "    step_state = event['Input']['Payload']\n",
    "    text_detection_job_id = step_state['textDetectionJobId']\n",
    "    table_analysis_job_id = step_state['tableAnalysisJobId']\n",
    "\n",
    "    # Check the status of the raw text extraction job\n",
    "    text_detection_status_response = textract_client.get_document_text_detection(JobId=text_detection_job_id)\n",
    "    text_detection_status = text_detection_status_response['JobStatus']\n",
    "\n",
    "    # Check the status of the table extraction job\n",
    "    table_analysis_status_response = textract_client.get_document_analysis(JobId=table_analysis_job_id)\n",
    "    table_analysis_status = table_analysis_status_response['JobStatus']\n",
    "\n",
    "    # Add detection and analysis status to the step state\n",
    "    step_state['textDetectionStatus'] = text_detection_status\n",
    "    step_state['tableAnalysisStatus'] = table_analysis_status\n",
    "\n",
    "    # Add TextractJobStatus key to the step state based on the status of both jobs\n",
    "    if text_detection_status == 'SUCCEEDED' and table_analysis_status == 'SUCCEEDED':\n",
    "        step_state['TextractJobStatus'] = 'COMPLETED'\n",
    "    elif text_detection_status == 'FAILED' or table_analysis_status == 'FAILED':\n",
    "        step_state['TextractJobStatus'] = 'FAILED'\n",
    "\n",
    "    # Return the status of both Textract jobs along with the S3 bucket and object key\n",
    "    return step_state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IAM\n",
    "More IAM roles and policies: \n",
    "\n",
    "For the role attached to doc-to-json-textract:\n",
    "Policy name: StartTextractTextDetectAndAnalysisAndGetPutS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"textract:StartDocumentTextDetection\",\n",
    "                \"textract:StartDocumentAnalysis\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app/*\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the role attached to textract-status-checker:\n",
    "Name: GetTextractTextDetectAndAnalysisObjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"textract:GetDocumentTextDetection\",\n",
    "                \"textract:GetDocumentAnalysis\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need this policy on status-checker too: \n",
    "Policy name: S3GetPutListGetLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:GetBucketLocation\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app\",\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app/*\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before this can be written we have to create the database that it will be put into, otherwise I will have no idea how to parse this. It will also need an IAM role and policy attached to it.\n",
    "\n",
    "Take  look at this: https://github.com/aws-samples/amazon-textract-response-parser\n",
    "\n",
    "First pass at parse-textract-json-obj-for-rds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the python will go here it is being developed in the other py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textract-trp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-textractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-prettyprinter==0.0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/bfaris96/Desktop/data_engineer/FCC-doc-tables/school_assessments_proj/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from trp import Document\n",
    "from textractprettyprinter.t_pretty_print import convert_table_to_list\n",
    "from textractcaller.t_call import call_textract, Textract_Features, call_textract_expense\n",
    "import trp\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "\n",
    "file = os.getenv('2019')\n",
    "\n",
    "resp = call_textract(input_document=file, features=[Textract_Features.TABLES])\n",
    "tdoc = Document(resp)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# iterating through to parse the items into dfs\n",
    "for page in tdoc.pages:\n",
    "    for table in page.tables:\n",
    "        tab_list = convert_table_to_list(trp_table=table)\n",
    "        dfs.append(pd.DataFrame(tab_list))\n",
    "\n",
    "\n",
    "# Now dfs is a list of DataFrames. Let's create a dictionary to store them:\n",
    "df_dict = {f'df{i+1}': df for i, df in enumerate(dfs)}\n",
    "\n",
    "# Now you can access the dataframes in df_dict like this:\n",
    "# df_dict['df1'], df_dict['df2'], etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df1':          0                         1   \n",
      "0              For each convenience   \\\n",
      "1                            MONDAY    \n",
      "2  WEEK 1   3 Entrée 1: 1 Entrée 2:    \n",
      "3  WEEK 2   3 Entrée 1: 1 Entrée 2:    \n",
      "4  WEEK 3   4 Entrée 1: 1 Entrée 2:    \n",
      "5  WEEK 4   4 Entrée 1: 1 Entrée 2:    \n",
      "6  WEEK 5       Entrée 1: Entrée 2:    \n",
      "7  WEEK 6       Entrée 1: Entrée 2:    \n",
      "\n",
      "                                              2   \n",
      "0  of the preparation = 1; minimal preparation   \\\n",
      "1                                      TUESDAY    \n",
      "2                      3 Entrée 1: 1 Entrée 2:    \n",
      "3                      3 Entrée 1: 1 Entrée 2:    \n",
      "4                      4 Entrée 1: 1 Entrée 2:    \n",
      "5                      3 Entrée 1: 2 Entrée 2:    \n",
      "6                          Entrée 1: Entrée 2:    \n",
      "7                          Entrée 1: Entrée 2:    \n",
      "\n",
      "                                            3   \n",
      "0  methods record the = 2; fast scratch = 3;   \\\n",
      "1                                  WEDNESDAY    \n",
      "2                    3 Entrée 1: 1 Entrée 2:    \n",
      "3                    3 Entrée 1: 1 Entrée 2:    \n",
      "4                    4 Entrée 1: 1 Entrée 2:    \n",
      "5                    3 Entrée 1: 1 Entrée 2:    \n",
      "6                        Entrée 1: Entrée 2:    \n",
      "7                        Entrée 1: Entrée 2:    \n",
      "\n",
      "                                              4                         5  \n",
      "0  corresponding number: made from scratch = 4                             \n",
      "1                                     THURSDAY                    FRIDAY   \n",
      "2                      3 Entrée 1: 1 Entrée 2:   3 Entrée 1: 1 Entrée 2:   \n",
      "3                      3 Entrée 1: 1 Entrée 2:   3 Entrée 1: 1 Entrée 2:   \n",
      "4                      3 Entrée 1: 2 Entrée 2:   3 Entrée 1: 1 Entrée 2:   \n",
      "5                      3 Entrée 1: 1 Entrée 2:   3 Entrée 1: 1 Entrée 2:   \n",
      "6                          Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "7                          Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   , 'df2':                       0              1                 2   \n",
      "0                        What are the   top 3 accepted?   \\\n",
      "1              Entrees     Vegetables            Fruits    \n",
      "2         Cheese Pizza          Fries      Strawberries    \n",
      "3  chicken and waffles       Broccoli       blueberries    \n",
      "4                Asian        Carrots             apple    \n",
      "\n",
      "                         3  \n",
      "0                           \n",
      "1  Whole Grain Rich Foods   \n",
      "2                   Pizza   \n",
      "3          Bread products   \n",
      "4                           , 'df3':                0                                                  1\n",
      "0  Grade Levels   Frequency (everyday, several times per week, r...\n",
      "1            NO                                                    \n",
      "2                                                                  \n",
      "3                                                                  \n",
      "4                                                                  , 'df4':                0                                                  1\n",
      "0  Grade Levels   Frequency (everyday, several times per week, r...\n",
      "1           1-4                             several times per week \n",
      "2                                                                  \n",
      "3                                                                  \n",
      "4                                                                  , 'df5':                                                     0   \n",
      "0          1. Review the scorecard before beginning.   \\\n",
      "1   2. Observe a lunch period. Check off statement...   \n",
      "2   3. Ask other school nutrition staff, teachers,...   \n",
      "3                                     FOCUS ON FRUIT    \n",
      "4   NOT_SELECTED, SELECTED, NOT_SELECTED, NOT_SELE...   \n",
      "5                                VARY THE VEGETABLES    \n",
      "6   SELECTED, SELECTED, SELECTED, SELECTED, SELECT...   \n",
      "7                                HIGHLIGHT THE SALAD    \n",
      "8   SELECTED, SELECTED, SELECTED, Pre-packaged sal...   \n",
      "9                               MOVE MORE WHITE MILK    \n",
      "10  SELECTED, NOT_SELECTED, NOT_SELECTED, SELECTED...   \n",
      "11                          BOOST REIMBURSABLE MEALS    \n",
      "12  SELECTED, Cafeteria staff politely prompt stud...   \n",
      "13  SELECTED, One entrée is identified as the feat...   \n",
      "14  SELECTED, Creative, descriptive names are used...   \n",
      "15  NOT_SELECTED, One reimbursable meal is identif...   \n",
      "16  NOT_SELECTED, The combo meal of the day or fea...   \n",
      "\n",
      "                                                    1  \n",
      "0                                4. Tally the score.   \n",
      "1   5. Discuss the results with stakeholders. Choo...  \n",
      "2                              SmarterLunchrooms.org   \n",
      "3                                                      \n",
      "4   SELECTED, NOT_SELECTED, NOT_SELECTED, At least...  \n",
      "5                                                      \n",
      "6   NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...  \n",
      "7                                                      \n",
      "8   NOT_SELECTED, Pre-packaged salads or salad bar...  \n",
      "9                                                      \n",
      "10  NOT_SELECTED, 1% or non-fat white milk is iden...  \n",
      "11                                                     \n",
      "12  NOT_SELECTED, NOT_SELECTED, A (reimbursable) c...  \n",
      "13  NOT_SELECTED, service line (e.g., a sign that ...  \n",
      "14  NOT_SELECTED, SELECTED, Students must use cash...  \n",
      "15  NOT_SELECTED, items if available.* Students ar...  \n",
      "16               Reimbursable Meals Subtotal 4 of 11   }\n"
     ]
    }
   ],
   "source": [
    "print(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WEEK_ENTREE score\n",
      "0      entrée1     1\n",
      "1      entrée2     3\n",
      "2      entrée1     1\n",
      "3      entrée2     3\n",
      "4      entrée1     1\n",
      "5      entrée2     3\n",
      "6      entrée1     1\n",
      "7      entrée2     3\n",
      "8      entrée1     1\n",
      "9      entrée1     1\n",
      "10     entrée2     3\n",
      "11     entrée1     1\n",
      "12     entrée2     3\n",
      "13     entrée1     1\n",
      "14     entrée2     3\n",
      "15     entrée1     1\n",
      "16     entrée2     3\n",
      "17     entrée1     1\n",
      "18     entrée1     1\n",
      "19     entrée2     4\n",
      "20     entrée1     1\n",
      "21     entrée2     4\n",
      "22     entrée1     1\n",
      "23     entrée2     3\n",
      "24     entrée1     2\n",
      "25     entrée2     3\n",
      "26     entrée1     1\n",
      "27     entrée1     1\n",
      "28     entrée2     3\n",
      "29     entrée1     2\n",
      "30     entrée2     3\n",
      "31     entrée1     1\n",
      "32     entrée2     3\n",
      "33     entrée1     1\n",
      "34     entrée2     3\n",
      "35     entrée1     1\n"
     ]
    }
   ],
   "source": [
    "# OKAY THIS WILL USEFULLY PARSE THE DF from the newer format, where score comes AFTER the week_entree\n",
    "# as in McAlester 22-23\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def process_dataframes(dfs_dict):\n",
    "    df_new = pd.DataFrame(columns=[\"WEEK_ENTREE\", \"score\"])\n",
    "\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = ' '.join(map(str, row.values))\n",
    "            matches = re.findall(r'(?:WEEK \\d+)?\\s*Entrée\\s*\\d:\\s*\\d', row_str)\n",
    "            for match in matches:\n",
    "                week_entree, score = re.split(r':\\s*', match)\n",
    "                week_entree = week_entree.lower().replace(\" \", \"\")\n",
    "                df_new = pd.concat([df_new, pd.DataFrame({\"WEEK_ENTREE\": [week_entree], \"score\": [score]})], ignore_index=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# Call the function\n",
    "df_new1 = process_dataframes(df_dict)\n",
    "\n",
    "# print the new dataframe\n",
    "print(df_new1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0                         1   \n",
      "0              For each convenience   \\\n",
      "1                            MONDAY    \n",
      "2  WEEK 1               Entrée 1: 3    \n",
      "3                       Entrée 2: 3    \n",
      "4  WEEK 2   Entrée 1: 3 Entrée 2: 2    \n",
      "5  WEEK 3   Entrée 1: 3 Entrée 2: 3    \n",
      "6  WEEK 4       Entrée 1: Entrée 2:    \n",
      "7  WEEK 5       Entrée 1: Entrée 2:    \n",
      "8  WEEK 6       Entrée 1: Entrée 2:    \n",
      "\n",
      "                                              2   \n",
      "0  of the preparation = 1; minimal preparation   \\\n",
      "1                                      TUESDAY    \n",
      "2                                  Entrée 1: 2    \n",
      "3                                  Entrée 2: 3    \n",
      "4                      Entrée 1: 3 Entrée 2: 2    \n",
      "5                      Entrée 1: 3 Entrée 2: 2    \n",
      "6                          Entrée 1: Entrée 2:    \n",
      "7                          Entrée 1: Entrée 2:    \n",
      "8                          Entrée 1: Entrée 2:    \n",
      "\n",
      "                                       3   \n",
      "0  methods record the = 2; fast scratch   \\\n",
      "1                             WEDNESDAY    \n",
      "2                           Entrée 1: 3    \n",
      "3                           Entrée 2: 2    \n",
      "4               Entrée 1: 3 Entrée 2: 3    \n",
      "5               Entrée 1: 3 Entrée 2: 2    \n",
      "6                   Entrée 1: Entrée 2:    \n",
      "7                   Entrée 1: Entrée 2:    \n",
      "8                   Entrée 1: Entrée 2:    \n",
      "\n",
      "                                               4                         5  \n",
      "0  corresponding number: = 3; made from scratch                       = 4   \n",
      "1                                      THURSDAY                    FRIDAY   \n",
      "2                                   Entrée 1: 3               Entrée 1: 3   \n",
      "3                                   Entrée 2: 2               Entrée 2: 2   \n",
      "4                       Entrée 1: 4 Entrée 2: 2   Entrée 1: 2 Entrée 2: 2   \n",
      "5                       Entrée 1: 3 Entrée 2: 2   Entrée 1: 3 Entrée 2: 2   \n",
      "6                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "7                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "8                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n"
     ]
    }
   ],
   "source": [
    "print(df_dict['df7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ENTREE score\n",
      "0   entrée1     1\n",
      "1   entrée2     3\n",
      "2   entrée1     1\n",
      "3   entrée2     3\n",
      "4   entrée1     1\n",
      "5   entrée2     3\n",
      "6   entrée1     1\n",
      "7   entrée2     3\n",
      "8   entrée1     1\n",
      "9   entrée1     1\n",
      "10  entrée2     3\n",
      "11  entrée1     1\n",
      "12  entrée2     3\n",
      "13  entrée1     1\n",
      "14  entrée2     3\n",
      "15  entrée1     1\n",
      "16  entrée2     3\n",
      "17  entrée1     1\n",
      "18  entrée1     1\n",
      "19  entrée2     4\n",
      "20  entrée1     1\n",
      "21  entrée2     4\n",
      "22  entrée1     1\n",
      "23  entrée2     3\n",
      "24  entrée1     2\n",
      "25  entrée2     3\n",
      "26  entrée1     1\n",
      "27  entrée1     1\n",
      "28  entrée2     3\n",
      "29  entrée1     2\n",
      "30  entrée2     3\n",
      "31  entrée1     1\n",
      "32  entrée2     3\n",
      "33  entrée1     1\n",
      "34  entrée2     3\n",
      "35  entrée1     1\n"
     ]
    }
   ],
   "source": [
    "#This works, but we're not going to use it, I just don't want to delete it incase I really screw up something later\n",
    "\n",
    "# #this one just leaves out the WEEK\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# def process_dataframes(dfs_dict):\n",
    "#     df_new = pd.DataFrame(columns=[\"ENTREE\", \"score\"])\n",
    "\n",
    "#     for df_name, df in dfs_dict.items():\n",
    "#         for _, row in df.iterrows():\n",
    "#             # Join the row values into a string for regex matching\n",
    "#             row_str = ' '.join(map(str, row.values))\n",
    "#             # regular expression to capture \"Entrée X: Y\"\n",
    "#             matches = re.findall(r'Entrée\\s*\\d:\\s*\\d', row_str)\n",
    "#             for match in matches:\n",
    "#                 entree, score = re.split(r':\\s*', match)\n",
    "#                 entree = entree.lower().replace(\" \", \"\")\n",
    "#                 df_new = pd.concat([df_new, pd.DataFrame({\"ENTREE\": [entree], \"score\": [score]})], ignore_index=True)\n",
    "\n",
    "#     return df_new\n",
    "\n",
    "\n",
    "# # Call the function\n",
    "# df_entrees = process_dataframes(df_dict)\n",
    "\n",
    "# # print the new dataframe\n",
    "# print(df_entrees)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLANNING\n",
    "\n",
    "I have to take a step back at this point and plan our data structures. Here are the tables we want--the titles and the column names. \"Pre\" meanse pretest and post means posttest, at beginnind and end of intervention. The top-3 foods were only captured in pretest, but can bea meaningful year over year nonetheless.\n",
    "\n",
    "## Bixby-2019-menu-analysis-pre\n",
    "entree, score\n",
    "(roughly 30-50 rows)\n",
    "\n",
    "## Bixby-2019-prep-methods-pre\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "## Bixby-2019-smart-total-pre\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "## Bixby-2019-top-3-pre\n",
    "entree, veg, fruit, grain \n",
    "(three rows max, nullable. Might consider pivoting axes so we have columns 1, 2, 3 and rows entree, veg, fruit, grain)\n",
    "\n",
    "------\n",
    "\n",
    "## Bixby-2019-menu-analysis-post\n",
    "entree, score\n",
    "(roughly 30-50 rows)\n",
    "\n",
    "## Bixby-2019-prep-methods-post\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "## Bixby-2019-smart-total-post\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "(no top3 post)\n",
    "\n",
    "------\n",
    "\n",
    "## Withing out intermediate lambdas, though I need names for the dataframes that correspond to the tables.\n",
    "\n",
    "menu_df \n",
    "=\n",
    "Bixby-2019-menu-analysis-pre\n",
    "entree, score\n",
    "(roughly 30-50 rows)\n",
    "\n",
    "prep_df\n",
    "=\n",
    "Bixby-2019-prep-methods-pre\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "smart_df\n",
    "=\n",
    "Bixby-2019-smart-total-pre\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "top3_df\n",
    "=\n",
    "Bixby-2019-top-3-pre\n",
    "entree, veg, fruit, grain \n",
    "(three rows max, nullable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_assessments_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
