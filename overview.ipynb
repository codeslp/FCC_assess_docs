{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog here: https://wordpress.com/post/brianfarish3.wordpress.com/168\n",
    "\n",
    "We are here: https://docs.aws.amazon.com/textract/latest/dg/api-async.html\n",
    "\n",
    "Diagram of this pipeline: https://app.diagrams.net/#Hcodeslp%2Faws_arch_diagrams%2Fmaster%2Ftextract-stepfns.drawio\n",
    "\n",
    "AWS has this which could be helpful to me in writing my code in the testract JSON to RDS Lambda.\n",
    "\n",
    "boto3 textract documentation: \n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#textract\n",
    "\n",
    "AWS recommends using an asynchronous function to process multiple page files. I'm going to use step functions. More details on that decision in the blog."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IAM\n",
    "The role will be named: step-functions-lambda-role. The tag is \"app\": \"document-parser\"\n",
    "I am creating an IAM role for the Lambda function that will allow Step Functions to access resources. The policy will be called \"AllowStepFunctionStateMachineStart\". It uses the AWS standard policy AWSLambdaRole, which generates cloudwatch logs. Also I am adding permission to start state machines with this policy: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"states:StartExecution\",\n",
    "            \"Resource\": \"arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEP FUNCTION\n",
    "I'm creating a step function called Doc-Parser-Pipeline and attaching the above role to it. I'm not turning on logs now, but may later, as I inevitably mess up. Maybe even tracing to with X-ray. Tagged with app:document-parser.\n",
    "\n",
    "State machine arn:  arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMBDA\n",
    "I'm creating a step function \"run-step-function-lambda\" tagged with app:document-parser.\n",
    "\n",
    "Adding environment variable:\n",
    "STATEMACHINEARN:arn:aws:states:us-east-1:415832459288:stateMachine:Doc-Parser-Pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3\n",
    "Creating document upload bucket. Not public. Tag app:document-parser\n",
    "Bucket name: \n",
    "doc-upload-bucket-doc-parser-app\n",
    "S3 BUCKET ARN:\n",
    "arn:aws:s3:::doc-upload-bucket-doc-parser-app\n",
    "\n",
    "Properties > event notifications > create\n",
    "Event name: \n",
    "trigger-document-parsing-event \n",
    "\n",
    "Suffix:\n",
    ".pdf\n",
    "\n",
    "Object creation, checking \"all object create events\". From the Lambda dropdown at the bottom selecting \"run-step-functions-lambda\"\n",
    "\n",
    "Creating an S3 bucket for textract's JSON object output objects used during the pipeline. No triggers on it for now.\n",
    "\n",
    "Name: \n",
    "textract-json-output-parser-app\n",
    "\n",
    "ARN:\n",
    "arn:aws:s3:::textract-json-output-parser-app\n",
    "\n",
    "We need a bocket policy on this last bucket: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Id\": \"Policy1683945477760\",\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "      {\n",
    "        \"Sid\": \"Stmt1683945475477\",\n",
    "        \"Action\": \"s3:PutObject\",\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Resource\": \"arn:aws:s3:::textract-json-output-parser-app/*\",\n",
    "        \"Principal\": {\n",
    "          \"AWS\": [\n",
    "            \"arn:aws:iam::415832459288:role/service-role/doc-to-json-textract-role-7n40j7gg\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMBDA\n",
    "\n",
    "We are going to create three Lambdas ***BUT*** they will not actually have the logic we need in them yet. They are basically placeholders. These are in order that they are invoked in the step function:\n",
    "\n",
    "Name:\n",
    "doc-to-json-textract\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:doc-to-json-textract\n",
    "\n",
    "Name:\n",
    "textract-status-checker\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:textract-status-checker\n",
    "\n",
    "Name:\n",
    "parse-textract-json-obj-for-rds\n",
    "ARN:\n",
    "arn:aws:lambda:us-east-1:415832459288:function:parse-textract-json-obj-for-rds\n",
    "\n",
    "# Reminder: At this point, all of these (and the initial run-step-functions-lambda also) are placeholders and contain no real logic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FUNCTIONS\n",
    "\n",
    "JSON will be pasted after this block.\n",
    "\n",
    "Editing - Deleting the default Hello and World states from the \"States\" key. \n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > doc-to-json-textract.\n",
    "\n",
    "Click copy to copy the invoke JSON and paste into definition code snippet after \"States\": {. Click format JSON to clean it up.\n",
    "Rename \"Invoke function\" to \"Start extraction\".  Make sure key above is \"StartAt\": \"Start Extraction\"\n",
    "\n",
    "Change Next key to \"Next:\" \"Wait for textract\"\n",
    "\n",
    "Add in Wait State. Change name to \"Wait for Textract\"\n",
    "\n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > textract-status-checker.  Change first line of new add from Invoke function to \"Check textract status\".  Make sure above key in Wait state is \"Next\": \"Check textract status\".\n",
    "\n",
    "Add a code snippet for a Choice state. Will be edited heavily see code below. Don't need a \"NOT\" block. (The default state later will go back to the Wait state.) Essentially two blocks with \"StringEquals\": \"COMPLETED\" or second with \"StringEquals\": \"FAILED\". The variable \"$.Payload.TextractJobStatus\".\n",
    "\n",
    "Change kv Default to \"Default\": \"Wait for textract\"\n",
    "\n",
    "Generate code snippet > Invoke a function\n",
    "Select fn from dropdown > parse-textract-json-obj-for-rds.  Change first line of new add from Invoke function to \"Parse textract JSON\".  Make sure above key in Wait state is \"Next\": \"Parse textract JSON\".\n",
    "\n",
    "In choices COMPLETED block set kv \"Next\": \"Parse textract JSON\".\n",
    "\n",
    "Change last \"Next\" to kv \"End\": true\n",
    "\n",
    "For FAILED Choice block:\n",
    "Generate snippet > flow control state > Fail state. Copy. Paste after END. Change as you see. Set FAILED block kv \"Next\": \"Textract failed\"\n",
    "\n",
    "Chart here: \n",
    "/Users/bfaris96/Desktop/data_engineer/FCC-doc-tables/school_assessments_proj/Screen Shot 2023-05-08 at 3.59.30 PM.png\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"run-step-function-lambda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_function_client = boto3.client('stepfunctions')\n",
    "    state_machine_arn = os.environ['STATEMACHINEARN']\n",
    "    \n",
    "    # Extract relevant information from the S3 event\n",
    "    s3_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    s3_object_key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    # Prepare the input for the state machine\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_object_key\n",
    "    }\n",
    "    \n",
    "    # Start the execution of the state machine\n",
    "    response = step_function_client.start_execution(\n",
    "        stateMachineArn=state_machine_arn,\n",
    "        input=json.dumps(step_state)\n",
    "    )\n",
    "\n",
    "    return json.dumps(response, default=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"doc-to-json-textract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    textract_client = boto3.client('textract')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']\n",
    "    s3_bucket = step_state['s3Bucket']   # this is the initial upload bucket\n",
    "    s3_object_key = step_state['s3ObjectKey']   # this is the initial upload object\n",
    "\n",
    "    # Prepare the output bucket\n",
    "    output_bucket = \"textract-json-output-parser-app\"\n",
    "\n",
    "    # Start the Textract job for the uploaded document using StartDocumentTextDetection API for extracting raw text\n",
    "    start_document_text_detection_response = textract_client.start_document_text_detection(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3_bucket,\n",
    "                'Name': s3_object_key\n",
    "            }\n",
    "        },\n",
    "        OutputConfig={\n",
    "            'S3Bucket': output_bucket,\n",
    "            'S3Prefix': f\"{s3_object_key}-text-detection/\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Start the Textract job for the uploaded document using StartDocumentAnalysis API for extracting tables\n",
    "    start_document_analysis_response = textract_client.start_document_analysis(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': s3_bucket,\n",
    "                'Name': s3_object_key\n",
    "            }\n",
    "        },\n",
    "        FeatureTypes=['TABLES'],\n",
    "        OutputConfig={\n",
    "            'S3Bucket': output_bucket,\n",
    "            'S3Prefix': f\"{s3_object_key}-table-analysis/\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Creating keys for the text detection and table analysis output objects\n",
    "    text_detection_output_prefix = f\"{s3_object_key}-text-detection/\"\n",
    "    table_analysis_output_prefix = f\"{s3_object_key}-table-analysis/\"\n",
    "    step_state['textDetectionOutputObjectKey'] = f\"{text_detection_output_prefix}{s3_object_key}.json\"\n",
    "    step_state['tableAnalysisOutputObjectKey'] = f\"{table_analysis_output_prefix}{s3_object_key}.json\"\n",
    "\n",
    "    # Update the step-state with the Textract JobIds\n",
    "    step_state['textDetectionJobId'] = start_document_text_detection_response['JobId']\n",
    "    step_state['tableAnalysisJobId'] = start_document_analysis_response['JobId']\n",
    "\n",
    "    return step_state\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass at \"textract-status-checker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    textract_client = boto3.client('textract')\n",
    "\n",
    "    # Retrieve the Textract JobIds, S3 bucket, and object key from the input event\n",
    "    step_state = event['Input']['Payload']\n",
    "    text_detection_job_id = step_state['textDetectionJobId']\n",
    "    table_analysis_job_id = step_state['tableAnalysisJobId']\n",
    "\n",
    "    # Check the status of the raw text extraction job\n",
    "    text_detection_status_response = textract_client.get_document_text_detection(JobId=text_detection_job_id)\n",
    "    text_detection_status = text_detection_status_response['JobStatus']\n",
    "\n",
    "    # Check the status of the table extraction job\n",
    "    table_analysis_status_response = textract_client.get_document_analysis(JobId=table_analysis_job_id)\n",
    "    table_analysis_status = table_analysis_status_response['JobStatus']\n",
    "\n",
    "    # Add detection and analysis status to the step state\n",
    "    step_state['textDetectionStatus'] = text_detection_status\n",
    "    step_state['tableAnalysisStatus'] = table_analysis_status\n",
    "\n",
    "    # Add TextractJobStatus key to the step state based on the status of both jobs\n",
    "    if text_detection_status == 'SUCCEEDED' and table_analysis_status == 'SUCCEEDED':\n",
    "        step_state['TextractJobStatus'] = 'COMPLETED'\n",
    "    elif text_detection_status == 'FAILED' or table_analysis_status == 'FAILED':\n",
    "        step_state['TextractJobStatus'] = 'FAILED'\n",
    "\n",
    "    # Return the status of both Textract jobs along with the S3 bucket and object key\n",
    "    return step_state\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IAM\n",
    "More IAM roles and policies: \n",
    "\n",
    "For the role attached to doc-to-json-textract:\n",
    "Policy name: StartTextractTextDetectAndAnalysisAndGetPutS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"textract:StartDocumentTextDetection\",\n",
    "                \"textract:StartDocumentAnalysis\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app/*\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the role attached to textract-status-checker:\n",
    "Name: GetTextractTextDetectAndAnalysisObjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"textract:GetDocumentTextDetection\",\n",
    "                \"textract:GetDocumentAnalysis\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we need this policy on status-checker too: \n",
    "Policy name: S3GetPutListGetLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:GetBucketLocation\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app\",\n",
    "                \"arn:aws:s3:::doc-upload-bucket-doc-parser-app/*\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app\",\n",
    "                \"arn:aws:s3:::textract-json-output-parser-app/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before this can be written we have to create the database that it will be put into, otherwise I will have no idea how to parse this. It will also need an IAM role and policy attached to it.\n",
    "\n",
    "Take  look at this: https://github.com/aws-samples/amazon-textract-response-parser\n",
    "\n",
    "First pass at parse-textract-json-obj-for-rds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the python will go here it is being developed in the other py files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLANNING\n",
    "\n",
    "I have to take a step back at this point and plan our data structures. Here are the tables we want--the titles and the column names. \"Pre\" meanse pretest and post means posttest, at beginnind and end of intervention. The top-3 foods were only captured in pretest, but can bea meaningful year over year nonetheless.\n",
    "\n",
    "## Bixby-2019-menu-analysis-pre\n",
    "entree, score\n",
    "(roughly 30-50 rows. These are probably all just going to get aggregated with a sum or a median)\n",
    "\n",
    "## Bixby-2019-variety-pre\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "## Bixby-2019-smart-total-pre\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "## Bixby-2019-top-3-pre\n",
    "entree, veg, fruit, grain \n",
    "(three rows max, nullable. Might consider pivoting axes so we have columns 1, 2, 3 and rows entree, veg, fruit, grain)\n",
    "\n",
    "------\n",
    "\n",
    "## Bixby-2019-menu-analysis-post\n",
    "entree, score\n",
    "(roughly 30-50 rows)\n",
    "\n",
    "## Bixby-2019-variety-post\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "## Bixby-2019-smart-total-post\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "(no top3 post)\n",
    "\n",
    "------\n",
    "\n",
    "## Within our intermediate lambdas, though I need names for the dataframes that correspond to the tables.\n",
    "\n",
    "menu_df \n",
    "=\n",
    "Bixby-2019-menu-analysis-pre\n",
    "entree, score\n",
    "(roughly 30-50 rows)\n",
    "\n",
    "variety_df\n",
    "=\n",
    "Bixby-2019-prep-methods-pre\n",
    "num_entree, num_veg, num_fruit\n",
    "(one row)\n",
    "\n",
    "smart_score\n",
    "=\n",
    "Bixby-2019-smart-total-pre\n",
    "smart_total \n",
    "(one number out of 60 max)\n",
    "\n",
    "top3_df\n",
    "=\n",
    "Bixby-2019-top-3-pre\n",
    "entree, veg, fruit, grain \n",
    "(three rows max, nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textract-trp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-textractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install amazon-textract-prettyprinter==0.0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/bfaris96/Desktop/data_engineer/FCC-doc-tables/school_assessments_proj/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "from trp import Document\n",
    "from textractprettyprinter.t_pretty_print import convert_table_to_list\n",
    "from textractcaller.t_call import call_textract, Textract_Features, call_textract_expense\n",
    "import trp\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "\n",
    "file = os.getenv('2022')\n",
    "\n",
    "table_rep = call_textract(input_document=file, features=[Textract_Features.TABLES])\n",
    "full_resp = call_textract(input_document=file)\n",
    "tdoc = Document(table_rep)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# iterating through to parse the items into dfs\n",
    "for page in tdoc.pages:\n",
    "    for table in page.tables:\n",
    "        tab_list = convert_table_to_list(trp_table=table)\n",
    "        dfs.append(pd.DataFrame(tab_list))\n",
    "\n",
    "\n",
    "# Now dfs is a list of DataFrames. Let's create a dictionary to store them:\n",
    "df_dict = {f'df{i+1}': df for i, df in enumerate(dfs)}\n",
    "\n",
    "# Now you can access the dataframes in df_dict like this:\n",
    "# df_dict['df1'], df_dict['df2'], etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df1':           0                                  1     2 3\n",
      "0             School Site Student Enrollment:   600   \n",
      "1  Percent   Free/Reduced-Price Meals Served:         , 'df2':                                                     0    1\n",
      "0                                 School Quick Facts    1 \n",
      "1                               Organizational Chart    2 \n",
      "2            Cooking for Kids Chef Consult Structure    4 \n",
      "3                               COVID-19 Adaptations    5 \n",
      "4   Build Collaborative Relationships: Instruction...   7 \n",
      "5           Entry Conference & Review of Application    8 \n",
      "6                 School Site's Meals per Labor Hour   10 \n",
      "7                                Preparation Methods   11 \n",
      "8                                      Menu Planning   13 \n",
      "9                                   Food Procurement   14 \n",
      "10                                      Meal Service   16 \n",
      "11                        School Nutrition Promotion   17 \n",
      "12                        Observations & Assessments   18 \n",
      "13                       C4K Mise en Place Scorecard   19 \n",
      "14               C4K Food Prep Observation Worksheet   21 \n",
      "15                    C4K Taste and Flavor Worksheet   22 \n",
      "16                        Professionalism Assessment   23 \n",
      "17                      Smarter Lunchrooms Scorecard   24 \n",
      "18                         General Observation Notes   27 \n",
      "19                                  Action Plan Prep   28 \n",
      "20                                   Chef Reflection   28 \n",
      "21                       Onsite Training Suggestions   30 \n",
      "22                      Smarter Lunchrooms Reference   31 , 'df3':                       0                           1\n",
      "0  Number of Employees   Total Labor Hours per Day \n",
      "1                    8                      (45.5) , 'df4':             0                             1                2        3\n",
      "0              Average Daily Participation   (Meals Served)          \n",
      "1  Breakfast                         Lunch            Snack   Supper \n",
      "2        240                           520                           , 'df5':                               0                                             1   \n",
      "0   Number of Meal Equivalents   Meals per Labor Hour (conventional systems)   \\\n",
      "1                    Up to 100                                             8    \n",
      "2                      101-150                                             9    \n",
      "3                      151-200                                         10-11    \n",
      "4                      201-250                                            12    \n",
      "5                      251-300                                            13    \n",
      "6                      301-400                                            14    \n",
      "7                      401-500                                            14    \n",
      "8                      501-600                                            15    \n",
      "9                      601-700                                            16    \n",
      "10                     701-800                                            17    \n",
      "11                        800+                                            18    \n",
      "\n",
      "                                2   \n",
      "0   Total Hours of labor per day   \\\n",
      "1                           9-12    \n",
      "2                          12-16    \n",
      "3                          16-17    \n",
      "4                          17-20    \n",
      "5                          20-22    \n",
      "6                          22-29    \n",
      "7                          35-40    \n",
      "8                          40-43    \n",
      "9                          40-43    \n",
      "10                         43-47    \n",
      "11                           47+    \n",
      "\n",
      "                                              3                              4  \n",
      "0   Meals per Labor Hour (convenience systems)   Total Hours of labor per day   \n",
      "1                                            9                           9-11   \n",
      "2                                           10                          11-14   \n",
      "3                                           12                          14-16   \n",
      "4                                           14                          16-18   \n",
      "5                                           15                          18-20   \n",
      "6                                           16                          20-25   \n",
      "7                                           18                          25-28   \n",
      "8                                           18                          28-34   \n",
      "9                                           19                          34-37   \n",
      "10                                          20                          37-40   \n",
      "11                                         21+                            40+   , 'df6':                             0                              1   \n",
      "0  # of entrée choices daily   # of vegetable choices daily   \\\n",
      "1                          2                              2    \n",
      "\n",
      "                           2  \n",
      "0  # of fruit choices daily   \n",
      "1                         1   , 'df7':          0                         1   \n",
      "0              For each convenience   \\\n",
      "1                            MONDAY    \n",
      "2  WEEK 1               Entrée 1: 3    \n",
      "3                       Entrée 2: 3    \n",
      "4  WEEK 2   Entrée 1: 3 Entrée 2: 2    \n",
      "5  WEEK 3   Entrée 1: 3 Entrée 2: 3    \n",
      "6  WEEK 4       Entrée 1: Entrée 2:    \n",
      "7  WEEK 5       Entrée 1: Entrée 2:    \n",
      "8  WEEK 6       Entrée 1: Entrée 2:    \n",
      "\n",
      "                                              2   \n",
      "0  of the preparation = 1; minimal preparation   \\\n",
      "1                                      TUESDAY    \n",
      "2                                  Entrée 1: 2    \n",
      "3                                  Entrée 2: 3    \n",
      "4                      Entrée 1: 3 Entrée 2: 2    \n",
      "5                      Entrée 1: 3 Entrée 2: 2    \n",
      "6                          Entrée 1: Entrée 2:    \n",
      "7                          Entrée 1: Entrée 2:    \n",
      "8                          Entrée 1: Entrée 2:    \n",
      "\n",
      "                                       3   \n",
      "0  methods record the = 2; fast scratch   \\\n",
      "1                             WEDNESDAY    \n",
      "2                           Entrée 1: 3    \n",
      "3                           Entrée 2: 2    \n",
      "4               Entrée 1: 3 Entrée 2: 3    \n",
      "5               Entrée 1: 3 Entrée 2: 2    \n",
      "6                   Entrée 1: Entrée 2:    \n",
      "7                   Entrée 1: Entrée 2:    \n",
      "8                   Entrée 1: Entrée 2:    \n",
      "\n",
      "                                               4                         5  \n",
      "0  corresponding number: = 3; made from scratch                       = 4   \n",
      "1                                      THURSDAY                    FRIDAY   \n",
      "2                                   Entrée 1: 3               Entrée 1: 3   \n",
      "3                                   Entrée 2: 2               Entrée 2: 2   \n",
      "4                       Entrée 1: 4 Entrée 2: 2   Entrée 1: 2 Entrée 2: 2   \n",
      "5                       Entrée 1: 3 Entrée 2: 2   Entrée 1: 3 Entrée 2: 2   \n",
      "6                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "7                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "8                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   , 'df8':           0            1        2                        3\n",
      "0  Entrees   Vegetables   Fruits   Whole Grain Rich Foods \n",
      "1                                                         \n",
      "2                                                         \n",
      "3                                                         , 'df9':                0                                                  1\n",
      "0  Grade Levels   Frequency (everyday, several times per week, r...\n",
      "1                                                                  \n",
      "2                                                                  \n",
      "3                                                                  \n",
      "4                                                                  , 'df10':                0                                                  1\n",
      "0  Grade Levels   Frequency (everyday, several times per week, r...\n",
      "1                                                                  \n",
      "2                                                                  \n",
      "3                                                                  \n",
      "4                                                                  , 'df11':                                                    0               1   \n",
      "0                                Procurement Method      Milk/Dairy   \\\n",
      "1                Small purchase procedures (quotes)   NOT_SELECTED,    \n",
      "2                           Competitive sealed bids   NOT_SELECTED,    \n",
      "3                             Competitive proposals       SELECTED,    \n",
      "4  Non-competitive negotiation (only used when co...  NOT_SELECTED,    \n",
      "\n",
      "                2               3               4                        5   \n",
      "0   Canned Goods    Frozen Foods           Bread   Paper Goods & Supplies   \\\n",
      "1  NOT_SELECTED,   NOT_SELECTED,   NOT_SELECTED,            NOT_SELECTED,    \n",
      "2  NOT_SELECTED,   NOT_SELECTED,   NOT_SELECTED,            NOT_SELECTED,    \n",
      "3      SELECTED,       SELECTED,       SELECTED,                SELECTED,    \n",
      "4  NOT_SELECTED,   NOT_SELECTED,   NOT_SELECTED,            NOT_SELECTED,    \n",
      "\n",
      "                       6  \n",
      "0  Equipment & Services   \n",
      "1         NOT_SELECTED,   \n",
      "2         NOT_SELECTED,   \n",
      "3             SELECTED,   \n",
      "4         NOT_SELECTED,   , 'df12':                             0                                 1   \n",
      "0     NOT_SELECTED, Hot Line           NOT_SELECTED, Salad Bar   \\\n",
      "1  NOT_SELECTED, Drink areas   NOT_SELECTED, Condiment options    \n",
      "\n",
      "                                    2                                      3  \n",
      "0           NOT_SELECTED, A la Carte   NOT_SELECTED, Other competitive food   \n",
      "1  NOT_SELECTED, Other service areas          NOT_SELECTED, A la Carte Menu   , 'df13':                  0                                                  1\n",
      "0       SELECTED,                                      Taste testing \n",
      "1       SELECTED,                                      Cooking demos \n",
      "2   NOT_SELECTED,                        Parent participation events \n",
      "3       SELECTED,                                       Art contests \n",
      "4       SELECTED,   Collaborate with classroom teachers on nutriti...\n",
      "5   NOT_SELECTED,                              School board meetings \n",
      "6   NOT_SELECTED,                                       Social media \n",
      "7   NOT_SELECTED,    Student Nutrition Advisory Council or Committee \n",
      "8       SELECTED,                                 Menu posted online \n",
      "9       SELECTED,                                       Menu printed \n",
      "10      SELECTED,                                 Menu announcements \n",
      "11      SELECTED,   Positive marketing or visual aids in the dinin...\n",
      "12  NOT_SELECTED,                             Other, please specify: , 'df14':                                         0       1\n",
      "0                                          Notes \n",
      "1                          Task Observed         \n",
      "2                Food or Food Items Used         \n",
      "3                         Equipment Used         \n",
      "4        Time Utilization and Management         \n",
      "5     Mise en Place (Written & Physical)         \n",
      "6         Techniques Used in Preparation         \n",
      "7  Suggestions for Maximizing Efficiency         \n",
      "8               Quality of Final Product         \n",
      "9             Personnel Name or Position         , 'df15':                            0                  1                   2   \n",
      "0              Food Quality   Unacceptable (1)   Below Average (2)   \\\n",
      "1              Presentation      NOT_SELECTED,       NOT_SELECTED,    \n",
      "2               Temperature      NOT_SELECTED,       NOT_SELECTED,    \n",
      "3             Taste/Balance      NOT_SELECTED,       NOT_SELECTED,    \n",
      "4                     Sauce      NOT_SELECTED,       NOT_SELECTED,    \n",
      "5                   Texture      NOT_SELECTED,       NOT_SELECTED,    \n",
      "6      Recipe/Menu Accuracy      NOT_SELECTED,       NOT_SELECTED,    \n",
      "7  Overall Recipe Execution      NOT_SELECTED,       NOT_SELECTED,    \n",
      "\n",
      "                3                   4               5  \n",
      "0    Average (3)   Above Average (4)   Excellent (5)   \n",
      "1  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   \n",
      "2      SELECTED,           SELECTED,   NOT_SELECTED,   \n",
      "3  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   \n",
      "4  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   \n",
      "5  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   \n",
      "6  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   \n",
      "7  NOT_SELECTED,       NOT_SELECTED,   NOT_SELECTED,   , 'df16':                                                     0   \n",
      "0                            Professionalism Quality   \\\n",
      "1                        Overall physical appearance    \n",
      "2                   Neat and clean uniform that fits    \n",
      "3          Styled hair and appropriate hair restrain    \n",
      "4                              Clean well-kept nails    \n",
      "5                     Well-kept slip resistant shoes    \n",
      "6    Deal with guests with on-going personal concern    \n",
      "7           Knowledge: Facts of the menu and options    \n",
      "8   Proficiency: Work to improve skills and add to...   \n",
      "9   Attentiveness: Knows the \"state\" of the servin...   \n",
      "10                       Preparedness: Mise en Place    \n",
      "11                      Efficiency: No \"empty hands\"    \n",
      "12                       Proper usage of side towels    \n",
      "13                             Proper usage of apron    \n",
      "14           Proper usage of thermometer and sharpie    \n",
      "15                                       No earrings    \n",
      "16                                      No bracelets    \n",
      "17       Watch and wedding ring is industry standard    \n",
      "18                                        Full pants    \n",
      "\n",
      "                                    1                          2   \n",
      "0   Does Not Meet Industry Standards   Meets Industry Standards   \\\n",
      "1                      NOT_SELECTED,                  SELECTED,    \n",
      "2                      NOT_SELECTED,                  SELECTED,    \n",
      "3                      NOT_SELECTED,                  SELECTED,    \n",
      "4                      NOT_SELECTED,                  SELECTED,    \n",
      "5                      NOT_SELECTED,                  SELECTED,    \n",
      "6                      NOT_SELECTED,                  SELECTED,    \n",
      "7                      NOT_SELECTED,                  SELECTED,    \n",
      "8                      NOT_SELECTED,                  SELECTED,    \n",
      "9                      NOT_SELECTED,                  SELECTED,    \n",
      "10                     NOT_SELECTED,                  SELECTED,    \n",
      "11                     NOT_SELECTED,                  SELECTED,    \n",
      "12                     NOT_SELECTED,                  SELECTED,    \n",
      "13                     NOT_SELECTED,                  SELECTED,    \n",
      "14                     NOT_SELECTED,                  SELECTED,    \n",
      "15                     NOT_SELECTED,                  SELECTED,    \n",
      "16                     NOT_SELECTED,                  SELECTED,    \n",
      "17                     NOT_SELECTED,                  SELECTED,    \n",
      "18                     NOT_SELECTED,                  SELECTED,    \n",
      "\n",
      "                              3  \n",
      "0   Exceeds Industry Standards   \n",
      "1                NOT_SELECTED,   \n",
      "2                NOT_SELECTED,   \n",
      "3                NOT_SELECTED,   \n",
      "4                NOT_SELECTED,   \n",
      "5                NOT_SELECTED,   \n",
      "6                NOT_SELECTED,   \n",
      "7                NOT_SELECTED,   \n",
      "8                NOT_SELECTED,   \n",
      "9                NOT_SELECTED,   \n",
      "10               NOT_SELECTED,   \n",
      "11               NOT_SELECTED,   \n",
      "12               NOT_SELECTED,   \n",
      "13               NOT_SELECTED,   \n",
      "14               NOT_SELECTED,   \n",
      "15               NOT_SELECTED,   \n",
      "16               NOT_SELECTED,   \n",
      "17               NOT_SELECTED,   \n",
      "18               NOT_SELECTED,   , 'df17':                                                    0   \n",
      "0                                                     \\\n",
      "1  SELECTED, SELECTED, At least two kinds of frui...   \n",
      "2  NOT_SELECTED, A variety of mixed whole fruits ...   \n",
      "3  SELECTED, Fruit is offered in at least two loc...   \n",
      "4  SELECTED, SELECTED, At least two kinds of vege...   \n",
      "5  SELECTED, SELECTED, Both hot and cold vegetabl...   \n",
      "6  SELECTED, A serving of vegetables is incorpora...   \n",
      "\n",
      "                                                   1  \n",
      "0                                                     \n",
      "1  NOT_SELECTED, At least one fruit is identified...  \n",
      "2  NOT_SELECTED, A fruit taste test is offered at...  \n",
      "3                Focus on the Fruit Subtotal 3 of 6   \n",
      "4  SELECTED, Self-serve spices and seasoning are ...  \n",
      "5  NOT_SELECTED, NOT_SELECTED, At least one veget...  \n",
      "6               Vary the Vegetables Subtotal 6 of 8   , 'df18':                                                    0   \n",
      "0  NOT_SELECTED, SELECTED, SELECTED, Pre-packaged...  \\\n",
      "1  SELECTED, SELECTED, SELECTED, Milk cases/coole...   \n",
      "2  NOT_SELECTED, SELECTED, NOT_SELECTED, SELECTED...   \n",
      "3  NOT_SELECTED, SELECTED, SELECTED, SELECTED, SE...   \n",
      "\n",
      "                                                   1  \n",
      "0  NOT_SELECTED, Pre-packaged salads or salad bar...  \n",
      "1  NOT_SELECTED, NOT_SELECTED, White milk is disp...  \n",
      "2  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...  \n",
      "3  SELECTED, SELECTED, SELECTED, SELECTED, SELECT...  , 'df19':                        0                 1   \n",
      "0    Smarter Lunchrooms   Scorecard Total   \\\n",
      "1        Focus on Fruit            3 of 6    \n",
      "2   Vary the Vegetables            6 of 8    \n",
      "3   Highlight the Salad            2 of 4    \n",
      "4  Move More White Milk            3 of 5    \n",
      "5    Reimbursable Meals           5 of 11    \n",
      "6  Lunchroom Atmosphere          10 of 10    \n",
      "7   Student Involvement            3 of 6    \n",
      "8    School Involvement           5 of 10    \n",
      "9       Scorecard Total          37 of 60    \n",
      "\n",
      "                                                   2  \n",
      "0                                       Award Level   \n",
      "1                                      Bronze 15-25   \n",
      "2  Great job! This lunchroom is off to a strong s...  \n",
      "3                                                     \n",
      "4                                      Silver 26-45   \n",
      "5  Excellent. Think of all the kids that are insp...  \n",
      "6                                        healthier!   \n",
      "7                                                     \n",
      "8                                        Gold 46-60   \n",
      "9  This lunchroom is making the most of the Smart...  , 'df20':                                                    0   \n",
      "0  1. Review the scorecard before beginning. 2. O...  \\\n",
      "1  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...   \n",
      "2                               VARY THE VEGETABLES    \n",
      "3  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...   \n",
      "4                               HIGHLIGHT THE SALAD    \n",
      "5  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, Pre-...   \n",
      "6                              MOVE MORE WHITE MILK    \n",
      "7  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...   \n",
      "8                          BOOST REIMBURSABLE MEALS    \n",
      "9  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...   \n",
      "\n",
      "                                                   1  \n",
      "0  4. Tally the score. 5. Discuss the results wit...  \n",
      "1  NOT_SELECTED, NOT_SELECTED, At least one fruit...  \n",
      "2                                                     \n",
      "3  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, Self...  \n",
      "4                                                     \n",
      "5  NOT_SELECTED, Pre-packaged salads or salad bar...  \n",
      "6                                                     \n",
      "7  NOT_SELECTED, 1% or non-fat white milk is iden...  \n",
      "8                                                     \n",
      "9  NOT_SELECTED, NOT_SELECTED, NOT_SELECTED, NOT_...  , 'df21':                        0       1\n",
      "0        Focus on Fruit    of 6 \n",
      "1   Vary the Vegetables    of 8 \n",
      "2   Highlight the Salad    of 4 \n",
      "3  Move More White Milk    of 5 \n",
      "4    Reimbursable Meals   of 11 \n",
      "5  Lunchroom Atmosphere   of 10 \n",
      "6   Student Involvement    of 6 \n",
      "7    School Involvement   of 10 \n",
      "8       Scorecard Total   of 60 }\n"
     ]
    }
   ],
   "source": [
    "print(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------MENU_DF------------\n",
      "\n",
      "      WEEK_ENTREE score\n",
      "0   week1entrée1     3\n",
      "1        entrée1     2\n",
      "2        entrée1     3\n",
      "3        entrée1     3\n",
      "4        entrée1     3\n",
      "5        entrée2     3\n",
      "6        entrée2     3\n",
      "7        entrée2     2\n",
      "8        entrée2     2\n",
      "9        entrée2     2\n",
      "10  week2entrée1     3\n",
      "11       entrée2     2\n",
      "12       entrée1     3\n",
      "13       entrée2     2\n",
      "14       entrée1     3\n",
      "15       entrée2     3\n",
      "16       entrée1     4\n",
      "17       entrée2     2\n",
      "18       entrée1     2\n",
      "19       entrée2     2\n",
      "20  week3entrée1     3\n",
      "21       entrée2     3\n",
      "22       entrée1     3\n",
      "23       entrée2     2\n",
      "24       entrée1     3\n",
      "25       entrée2     2\n",
      "26       entrée1     3\n",
      "27       entrée2     2\n",
      "28       entrée1     3\n",
      "29       entrée2     2 \n",
      "\n",
      "\n",
      "--------------SMART_SCORE_2019------------\n",
      "\n",
      " None \n",
      "\n",
      "\n",
      "--------------SMART_SCORE_2020------------\n",
      "\n",
      " 37 \n",
      "\n",
      "\n",
      "--------------VARIETY_DF------------\n",
      "\n",
      "         food num_avail\n",
      "0     entrée        2 \n",
      "1  vegetable        2 \n",
      "2      fruit        1  \n",
      "\n",
      "\n",
      "--------------TOP3_DF_2019------------\n",
      "\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [] \n",
      "\n",
      "\n",
      "--------------TOP3_DF_2020------------\n",
      "\n",
      " 0 entrees vegetables fruits whole grain rich foods\n",
      "0                                                 \n",
      "1                                                 \n",
      "2                                                  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### PARSING FOR ALL THE DFs HERE#\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def process_menu_df(dfs_dict):\n",
    "    df_new = pd.DataFrame(columns=[\"WEEK_ENTREE\", \"score\"])\n",
    "\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        for _, row in df.iterrows():\n",
    "            row_str = ' '.join(map(str, row.values))\n",
    "            matches = re.findall(r'(?:WEEK \\d+)?\\s*Entrée\\s*\\d:\\s*\\d', row_str)\n",
    "            for match in matches:\n",
    "                week_entree, score = re.split(r':\\s*', match)\n",
    "                week_entree = week_entree.lower().replace(\" \", \"\")\n",
    "                df_new = pd.concat([df_new, pd.DataFrame({\"WEEK_ENTREE\": [week_entree], \"score\": [score]})], ignore_index=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "# Call the function\n",
    "menu_df = process_menu_df(df_dict)\n",
    "\n",
    "# This will return smart score for 2019, using the full response object.\n",
    "\n",
    "def process_smart_score_2019(full_response_object):\n",
    "    blocks = full_response_object['Blocks']\n",
    "    score = None\n",
    "    flag = False\n",
    "    for block in blocks:\n",
    "        if block['BlockType'] == 'LINE':\n",
    "            if flag and block['Text'].isdigit():\n",
    "                score = block['Text']\n",
    "                break\n",
    "            if block['Text'] == 'Scorecard Total':\n",
    "                flag = True\n",
    "    return score\n",
    "\n",
    "smart_score_2019 = process_smart_score_2019(full_resp)\n",
    "\n",
    "\n",
    "\n",
    "def process_smart_score_2020(dfs_dict):\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        # Ensure all column names are strings and convert them to lowercase\n",
    "        df.columns = df.columns.astype(str).str.lower()\n",
    "        if 'scorecard total' in df.columns:\n",
    "            last_cell = df['scorecard total'].str.lower().iloc[-1]\n",
    "            smart_score_2020 = last_cell.split(' of ')[0]\n",
    "            return int(smart_score_2020)\n",
    "    return None\n",
    "\n",
    "smart_score_2020 = process_smart_score_2020(df_dict)\n",
    "\n",
    "\n",
    "\n",
    "def process_prep_df(dfs_dict):\n",
    "    df_new = pd.DataFrame()\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        if \"# of\" in df.iloc[0, 0]:\n",
    "            for i in range(len(df.columns)):\n",
    "                col = df.iloc[:, i]\n",
    "                if \"# of\" in col[0]:\n",
    "                    food = col[0].replace(\"# of\", \"\").replace(\"choices daily\", \"\").strip()\n",
    "                    num = col[1]\n",
    "                    df_new = pd.concat([df_new, pd.DataFrame({\"food\": [food], \"num_avail\": [num]})], ignore_index=True)\n",
    "    return df_new\n",
    "\n",
    "variety_df = process_prep_df(df_dict)\n",
    "\n",
    "#   THis will work for 2020 and later:\n",
    "def process_food_df(dfs_dict):\n",
    "    df_new = pd.DataFrame()\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        df.columns = df.iloc[0].str.lower().str.strip()\n",
    "        df = df[1:]\n",
    "        if \"entrees\" in df.columns:\n",
    "            df_new = pd.concat([df_new, df.reset_index(drop=True)], ignore_index=True)\n",
    "    return df_new\n",
    "\n",
    "top3_df_2020 = process_food_df(df_dict)\n",
    "\n",
    "def process_food_df(dfs_dict):\n",
    "    df_new = pd.DataFrame()\n",
    "    for df_name, df in dfs_dict.items():\n",
    "        for i in range(len(df)):\n",
    "            row_values = df.iloc[i, :].values\n",
    "            if \"Entrees\" in row_values or \"what are the\" in ' '.join(map(str, row_values)).lower():\n",
    "                cols = df.iloc[i, :].reset_index(drop=True)\n",
    "                df_new = pd.concat([df_new, df.iloc[i+1:, :].reset_index(drop=True)], ignore_index=True)\n",
    "                df_new.columns = cols\n",
    "    return df_new\n",
    "\n",
    "top3_df_2019 = process_food_df(df_dict)\n",
    "\n",
    "\n",
    "# print the new dataframes\n",
    "print(\"--------------MENU_DF------------\\n\\n\", menu_df, \"\\n\\n\")\n",
    "print(\"--------------SMART_SCORE_2019------------\\n\\n\", smart_score_2019, \"\\n\\n\")\n",
    "print(\"--------------SMART_SCORE_2020------------\\n\\n\", smart_score_2020, \"\\n\\n\")\n",
    "print(\"--------------VARIETY_DF------------\\n\\n\", variety_df, \"\\n\\n\")\n",
    "print(\"--------------TOP3_DF_2019------------\\n\\n\", top3_df_2019, \"\\n\\n\")\n",
    "print(\"--------------TOP3_DF_2020------------\\n\\n\", top3_df_2020, \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023-05-16 test outputDFs are pasted in this file: /Users/bfaris96/Desktop/data_engineer/FCC-doc-tables/school_assessments_proj/OUTPUT_DFs.md"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right now the problem we have with the above function arises from the fact that the documents are laid out differently between years in some very small ways that are tricky for textract to parse and so it reverses the order of the entrees some times. This is not great, but is not a problem if we aggregate them. Which we probably will. And may have to because I can't figure out how to do it better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0                         1   \n",
      "0              For each convenience   \\\n",
      "1                            MONDAY    \n",
      "2  WEEK 1               Entrée 1: 3    \n",
      "3                       Entrée 2: 3    \n",
      "4  WEEK 2   Entrée 1: 3 Entrée 2: 2    \n",
      "5  WEEK 3   Entrée 1: 3 Entrée 2: 3    \n",
      "6  WEEK 4       Entrée 1: Entrée 2:    \n",
      "7  WEEK 5       Entrée 1: Entrée 2:    \n",
      "8  WEEK 6       Entrée 1: Entrée 2:    \n",
      "\n",
      "                                              2   \n",
      "0  of the preparation = 1; minimal preparation   \\\n",
      "1                                      TUESDAY    \n",
      "2                                  Entrée 1: 2    \n",
      "3                                  Entrée 2: 3    \n",
      "4                      Entrée 1: 3 Entrée 2: 2    \n",
      "5                      Entrée 1: 3 Entrée 2: 2    \n",
      "6                          Entrée 1: Entrée 2:    \n",
      "7                          Entrée 1: Entrée 2:    \n",
      "8                          Entrée 1: Entrée 2:    \n",
      "\n",
      "                                       3   \n",
      "0  methods record the = 2; fast scratch   \\\n",
      "1                             WEDNESDAY    \n",
      "2                           Entrée 1: 3    \n",
      "3                           Entrée 2: 2    \n",
      "4               Entrée 1: 3 Entrée 2: 3    \n",
      "5               Entrée 1: 3 Entrée 2: 2    \n",
      "6                   Entrée 1: Entrée 2:    \n",
      "7                   Entrée 1: Entrée 2:    \n",
      "8                   Entrée 1: Entrée 2:    \n",
      "\n",
      "                                               4                         5  \n",
      "0  corresponding number: = 3; made from scratch                       = 4   \n",
      "1                                      THURSDAY                    FRIDAY   \n",
      "2                                   Entrée 1: 3               Entrée 1: 3   \n",
      "3                                   Entrée 2: 2               Entrée 2: 2   \n",
      "4                       Entrée 1: 4 Entrée 2: 2   Entrée 1: 2 Entrée 2: 2   \n",
      "5                       Entrée 1: 3 Entrée 2: 2   Entrée 1: 3 Entrée 2: 2   \n",
      "6                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "7                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n",
      "8                           Entrée 1: Entrée 2:       Entrée 1: Entrée 2:   \n"
     ]
    }
   ],
   "source": [
    "print(df_dict['df7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works, but we're not going to use it, I just don't want to delete it incase I really screw up something later\n",
    "\n",
    "# #this one just leaves out the WEEK\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# def process_dataframes(dfs_dict):\n",
    "#     df_new = pd.DataFrame(columns=[\"ENTREE\", \"score\"])\n",
    "\n",
    "#     for df_name, df in dfs_dict.items():\n",
    "#         for _, row in df.iterrows():\n",
    "#             # Join the row values into a string for regex matching\n",
    "#             row_str = ' '.join(map(str, row.values))\n",
    "#             # regular expression to capture \"Entrée X: Y\"\n",
    "#             matches = re.findall(r'Entrée\\s*\\d:\\s*\\d', row_str)\n",
    "#             for match in matches:\n",
    "#                 entree, score = re.split(r':\\s*', match)\n",
    "#                 entree = entree.lower().replace(\" \", \"\")\n",
    "#                 df_new = pd.concat([df_new, pd.DataFrame({\"ENTREE\": [entree], \"score\": [score]})], ignore_index=True)\n",
    "\n",
    "#     return df_new\n",
    "\n",
    "\n",
    "# # Call the function\n",
    "# df_entrees = process_dataframes(df_dict)\n",
    "\n",
    "# # print the new dataframe\n",
    "# print(df_entrees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_assessments_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
