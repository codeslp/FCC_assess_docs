{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_function_client = boto3.client('stepfunctions')\n",
    "    state_machine_arn = os.environ['STATEMACHINEARN']\n",
    "    \n",
    "    # Extract relevant information from the S3 event\n",
    "    s3_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    s3_object_key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    # Prepare the input for the state machine\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_object_key\n",
    "    }\n",
    "    \n",
    "    # Start the execution of the state machine\n",
    "    response = step_function_client.start_execution(\n",
    "        stateMachineArn=state_machine_arn,\n",
    "        input=json.dumps(step_state)\n",
    "    )\n",
    "    print(json.dumps(step_state))\n",
    "    return step_state#, json.dumps(response, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### doc-to-json-textract LAMBDA ######\n",
    "#### \"Start extraction\" step in step function #####\n",
    "##### USING CALL_TEXTRACT FUNCTION #####\n",
    "##### JUST GETTING TABLES #####\n",
    "\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(json.dumps(event))\n",
    "    textract_client = boto3.client('textract', region_name='us-east-1')\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']\n",
    "    s3_bucket = step_state['s3Bucket']   # this is the initial upload bucket\n",
    "    s3_document_key = step_state['s3ObjectKey']   # this is the initial upload object\n",
    "\n",
    "    # Create the document uri:\n",
    "    s3_uri_of_document = f's3://{s3_bucket}/{s3_document_key}'\n",
    "\n",
    "    textract_json = call_textract(input_document=s3_uri_of_document, features=[Textract_Features.FORMS, Textract_Features.TABLES], boto3_textract_client = textract_client)\n",
    "    \n",
    "    # Save the JSON to S3\n",
    "    s3_textract_json_key = s3_document_key + \"_textract.json\"\n",
    "    s3_client.put_object(Body=json.dumps(textract_json), Bucket=s3_bucket, Key=s3_textract_json_key)\n",
    "\n",
    "    # Update step_state with the S3 key of textractJson\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_document_key,\n",
    "        \"s3TextractJsonKey\": s3_textract_json_key\n",
    "    }\n",
    "\n",
    "    # This returns the updated step_state back to the state machine.\n",
    "    return step_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### THIS IS PARSE_TEXTRACT_JSON_OBJ_FOR_RDS LAMBDA #####\n",
    "###### \"Parse Textract JSON\" step in step function #####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from trp import Document\n",
    "import boto3\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "def textract_to_dataframes(textract_json):\n",
    "    doc = Document(textract_json)\n",
    "    \n",
    "    df_dict = {}\n",
    "    table_counter = 1\n",
    "    \n",
    "    for page in doc.pages:\n",
    "        for table in page.tables:\n",
    "            table_rows = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "            df = pd.DataFrame(table_rows)\n",
    "            df_dict[f'df{table_counter}'] = df\n",
    "            table_counter += 1\n",
    "    print(df_dict)\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(json.dumps(event))\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']['Payload']\n",
    "    s3_bucket = step_state['s3Bucket']   \n",
    "    s3_textract_json_key = step_state['s3TextractJsonKey']\n",
    "    s3_object_key = step_state['s3ObjectKey']\n",
    "\n",
    "    # Define validation bucket\n",
    "    validation_bucket = \"validation-bucket---doc-parser\"\n",
    "\n",
    "    # Get the Textract JSON from S3\n",
    "    textract_json_object = s3_client.get_object(Bucket=s3_bucket, Key=s3_textract_json_key)\n",
    "    textract_json = json.load(textract_json_object['Body'])\n",
    "\n",
    "    # Convert the JSON to DataFrames\n",
    "    df_dict = textract_to_dataframes(textract_json)\n",
    "\n",
    "    # ###### TAKE THIS OUT AFTER DEBUGGIN DONE     #####\n",
    "    # # Write each DataFrame to a CSV in the validation bucket\n",
    "    # for df_name, df in df_dict.items():\n",
    "    #     # Generate a timestamp for the filename\n",
    "    #     timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #     # Generate the filename using the DataFrame name and the timestamp\n",
    "    #     csv_filename = f\"{df_name}-{timestamp}.csv\"\n",
    "    #     # Convert the DataFrame to CSV\n",
    "    #     csv_string = df.to_csv(index=False)\n",
    "    #     # Upload the CSV to S3\n",
    "    #     s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=csv_filename)\n",
    "\n",
    "\n",
    "\n",
    "    # Write DataFrame to \"menu.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains('WEEK 1').any():\n",
    "            # remove first row\n",
    "            fixed_df = df.copy().drop(df.index[0])\n",
    "            # Strip extra spaces from column names and cells m=\"\"\n",
    "            fixed_df = fixed_df.applymap(str.strip)\n",
    "            fixed_df.columns = fixed_df.columns.astype(str).str.strip()\n",
    "            fixed_df = fixed_df.drop(fixed_df.index[0])\n",
    "            \n",
    "            # Rename columns\n",
    "            new_columns = ['WEEK_NUM', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "            fixed_df.columns = new_columns[:len(fixed_df.columns)]\n",
    "\n",
    "            # Replace empty strings with np.nan\n",
    "            fixed_df.replace(\"\", np.nan, inplace=True)\n",
    "            \n",
    "            # Fill missing values in 'WEEK_NUM' column\n",
    "            fixed_df['WEEK_NUM'].ffill(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            csv_string = fixed_df.to_csv(index=False)\n",
    "            # create the menu.csv key with s3ObjectKey\n",
    "            menu_key = f\"{s3_object_key}_menu.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=menu_key)\n",
    "            step_state[\"menuCsvKey\"] = menu_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "\n",
    "    # Write DataFrame to \"food.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains('Entrees').any():\n",
    "            # Strip extra spaces from column names and cells\n",
    "            df = df.applymap(str.strip)\n",
    "            df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "            # Set column names to first row's values\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "\n",
    "            csv_string = df.to_csv(index=False)\n",
    "            # create the food.csv key with s3ObjectKey\n",
    "            food_key = f\"{s3_object_key}_food.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=food_key)\n",
    "            step_state[\"foodCsvKey\"] = food_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "\n",
    "    # Write DataFrame to \"choices.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains(\"# of entrÃ©e choices daily\").any():\n",
    "            # Strip extra spaces from column names and cells\n",
    "            df = df.applymap(str.strip)\n",
    "            df.columns = df.columns.astype(str).str.strip()\n",
    "            \n",
    "            # Set column names to first row's values\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "\n",
    "            csv_string = df.to_csv(index=False)\n",
    "            # create the choices.csv key with s3ObjectKey\n",
    "            choices_key = f\"{s3_object_key}_choices.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=choices_key)\n",
    "            step_state[\"choicesCsvKey\"] = choices_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "\n",
    "    # Add validation bucket to step_state\n",
    "    step_state[\"validationBucket\"] = validation_bucket\n",
    "\n",
    "    print(json.dumps(step_state, default=str))\n",
    "    # This returns the updated step_state back to the state machine.\n",
    "    return step_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### \"Validate table\" step in step function #####\n",
    "####### THIS IS table_validator LAMBDA #####\n",
    "\n",
    "\n",
    "##### (not inserted in lambda function yet) #####\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_state = event['Input']['Payload']\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    validation_bucket = step_state['validationBucket']\n",
    "    validation_key = 'validation.csv'  # Specify your validation csv key\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Try to load existing validation data from S3\n",
    "        validation_object = s3_client.get_object(Bucket=validation_bucket, Key=validation_key)\n",
    "        validation_df = pd.read_csv(validation_object['Body'])\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        # If validation.csv does not exist yet, create an empty DataFrame\n",
    "        validation_df = pd.DataFrame(columns=[\"Timestamp\", \"MenuIsValid\", \"FoodIsValid\", \"ChoicesIsValid\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########## MENU validation ###############\n",
    "\n",
    "\n",
    "    try:\n",
    "        menu_csv_key = step_state['menuCsvKey']\n",
    "        menu_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=menu_csv_key)\n",
    "        menu_df = pd.read_csv(menu_csv_object['Body'])\n",
    "\n",
    "        if menu_df.shape[0] >= 1 and menu_df.shape[1] == 6:\n",
    "            step_state[\"menuIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"menuIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"menuIsValid\"] = False\n",
    "        print(\"MenuCsvKeyError\")\n",
    "\n",
    "    \n",
    "    ########## FOOD validation ###############\n",
    "\n",
    "    try:\n",
    "        food_csv_key = step_state['foodCsvKey']\n",
    "        food_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=food_csv_key)\n",
    "        food_df = pd.read_csv(food_csv_object['Body'])\n",
    "\n",
    "        if food_df.shape[0] >= 1 and food_df.shape[1] == 4 and food_df.iloc[0].astype(str).str.contains(r'[a-zA-Z]').any():\n",
    "            step_state[\"foodIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"foodIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"foodIsValid\"] = False\n",
    "        print(\"FoodCsvKeyError\")\n",
    "\n",
    "\n",
    "\n",
    "    ########## CHOICES validation ###############\n",
    "\n",
    "    try:\n",
    "        choices_csv_key = step_state['choicesCsvKey']\n",
    "        choices_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=choices_csv_key)\n",
    "        choices_df = pd.read_csv(choices_csv_object['Body'])\n",
    "\n",
    "        if choices_df.shape[0] == 1 and choices_df.shape[1] == 3:\n",
    "            step_state[\"choicesIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"choicesIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"choicesIsValid\"] = False\n",
    "        print(\"ChoicesCsvKeyError\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Add new validation results\n",
    "    new_results = {\n",
    "        \"Timestamp\": datetime.datetime.now(),\n",
    "        \"MenuIsValid\": step_state[\"menuIsValid\"],\n",
    "        \"FoodIsValid\": step_state[\"foodIsValid\"],\n",
    "        \"ChoicesIsValid\": step_state[\"choicesIsValid\"]\n",
    "    }\n",
    "\n",
    "    new_results_df = pd.DataFrame([new_results])\n",
    "\n",
    "    validation_df = pd.concat([validation_df, new_results_df])\n",
    "\n",
    "    # Save the updated validation_df back to S3\n",
    "    csv_buffer = StringIO()\n",
    "    validation_df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=validation_bucket, Key=validation_key)\n",
    "\n",
    "    return step_state"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
