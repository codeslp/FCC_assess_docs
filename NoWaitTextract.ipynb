{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_function_client = boto3.client('stepfunctions')\n",
    "    state_machine_arn = os.environ['STATEMACHINEARN']\n",
    "    \n",
    "    # Extract relevant information from the S3 event\n",
    "    s3_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    s3_object_key = event['Records'][0]['s3']['object']['key']\n",
    "    \n",
    "    # Prepare the input for the state machine\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_object_key\n",
    "    }\n",
    "    \n",
    "    # Start the execution of the state machine\n",
    "    response = step_function_client.start_execution(\n",
    "        stateMachineArn=state_machine_arn,\n",
    "        input=json.dumps(step_state)\n",
    "    )\n",
    "    print(json.dumps(step_state))\n",
    "    return step_state#, json.dumps(response, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### doc-to-json-textract LAMBDA ######\n",
    "#### \"Start extraction\" step in step function #####\n",
    "##### USING CALL_TEXTRACT FUNCTION #####\n",
    "##### JUST GETTING TABLES #####\n",
    "\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from textractcaller.t_call import call_textract, Textract_Features\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(json.dumps(event))\n",
    "    textract_client = boto3.client('textract', region_name='us-east-1')\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']\n",
    "    s3_bucket = step_state['s3Bucket']   # this is the initial upload bucket\n",
    "    s3_document_key = step_state['s3ObjectKey']   # this is the initial upload object\n",
    "\n",
    "    # Create the document uri:\n",
    "    s3_uri_of_document = f's3://{s3_bucket}/{s3_document_key}'\n",
    "\n",
    "    textract_json = call_textract(input_document=s3_uri_of_document, features=[Textract_Features.FORMS, Textract_Features.TABLES], boto3_textract_client = textract_client)\n",
    "    \n",
    "    # Save the JSON to S3\n",
    "    s3_textract_json_key = s3_document_key + \"_textract.json\"\n",
    "    s3_client.put_object(Body=json.dumps(textract_json), Bucket=s3_bucket, Key=s3_textract_json_key)\n",
    "\n",
    "    # Update step_state with the S3 key of textractJson\n",
    "    step_state = {\n",
    "        \"s3Bucket\": s3_bucket,\n",
    "        \"s3ObjectKey\": s3_document_key,\n",
    "        \"s3TextractJsonKey\": s3_textract_json_key\n",
    "    }\n",
    "\n",
    "    # This returns the updated step_state back to the state machine.\n",
    "    return step_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### THIS IS PARSE_TEXTRACT_JSON_OBJ_FOR_RDS LAMBDA #####\n",
    "###### \"Parse Textract JSON\" step in step function #####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from trp import Document\n",
    "import boto3\n",
    "import datetime\n",
    "import uuid\n",
    "import re\n",
    "\n",
    "def textract_to_dataframes(textract_json):\n",
    "    doc = Document(textract_json)\n",
    "    \n",
    "    df_dict = {}\n",
    "    table_counter = 1\n",
    "    \n",
    "    for page in doc.pages:\n",
    "        for table in page.tables:\n",
    "            table_rows = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "            df = pd.DataFrame(table_rows)\n",
    "            df_dict[f'df{table_counter}'] = df\n",
    "            table_counter += 1\n",
    "    print(df_dict)\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(json.dumps(event))\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    step_state = event['Input']['Payload']\n",
    "    s3_bucket = step_state['s3Bucket']   \n",
    "    s3_textract_json_key = step_state['s3TextractJsonKey']\n",
    "    s3_object_key = step_state['s3ObjectKey']\n",
    "\n",
    "    # Define validation bucket\n",
    "    validation_bucket = \"validation-bucket---doc-parser\"\n",
    "\n",
    "    # Get the Textract JSON from S3\n",
    "    textract_json_object = s3_client.get_object(Bucket=s3_bucket, Key=s3_textract_json_key)\n",
    "    textract_json = json.load(textract_json_object['Body'])\n",
    "\n",
    "    # Convert the JSON to DataFrames\n",
    "    df_dict = textract_to_dataframes(textract_json)\n",
    "\n",
    "    \n",
    "    # Find first occurrence of the years 2018-2022 in s3_object_key\n",
    "    match = re.search(r'2018|2019|2020|2021|2022', s3_object_key)\n",
    "    year = match.group() if match else 'N/A'\n",
    "\n",
    "    # Find school name in s3_object_key\n",
    "    match = re.search(r'bixby|mcalester|altus|middleberg|middleburg|newcastle|thomas|wynona', s3_object_key.lower())\n",
    "    school = match.group() if match else 'N/A'\n",
    "\n",
    "\n",
    "    match = re.search(r'pre|post', s3_object_key.lower())\n",
    "    pre_post = match.group() if match else 'N/A'\n",
    "\n",
    "\n",
    "\n",
    "    ########################## Write DataFrame to \"menu.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains('WEEK 1').any():\n",
    "\n",
    "            # Create entree 2 empty df\n",
    "            entree_2_df = pd.DataFrame()\n",
    "            new_columns = ['week_num', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "            entree_2_df.columns = new_columns[:len(entree_2_df.columns)]\n",
    "\n",
    "            # Remove first row\n",
    "            fixed_df = df.copy().drop(df.index[0])\n",
    "\n",
    "            # Strip extra spaces from column names and cells\n",
    "            fixed_df = fixed_df.applymap(str.strip)\n",
    "            fixed_df.columns = fixed_df.columns.astype(str).str.strip()\n",
    "            fixed_df = fixed_df.drop(fixed_df.index[0])\n",
    "\n",
    "            # Rename columns\n",
    "            new_columns = ['week_num', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "            fixed_df.columns = new_columns[:len(fixed_df.columns)]\n",
    "\n",
    "            # Replace empty strings with np.nan\n",
    "            fixed_df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "            # Fill missing values in 'WEEK_NUM' column\n",
    "            fixed_df['week_num'].ffill(inplace=True)\n",
    "\n",
    "            # Delete any instances of \"Entrée\" and the rest of the string till a digit and a colon appears\n",
    "            for index, row in fixed_df.iterrows():\n",
    "                for col in fixed_df.columns:\n",
    "                    if pd.notnull(row[col]): \n",
    "                        fixed_df.at[index, col] = re.sub(r'Entrée[^0-9]*[0-9]:\\s*', '', row[col])\n",
    "\n",
    "            # Remove rows that don't contain any alphanumeric characters in the weekday columns\n",
    "            weekday_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday']\n",
    "            fixed_df = fixed_df[fixed_df[weekday_columns].apply(lambda row: row.astype(str).str.strip().str.contains('[A-Za-z0-9]').any(), axis=1)]\n",
    "            \n",
    "\n",
    "\n",
    "            def splitDataFrameList(df,target_column,separator):\n",
    "                ''' df = dataframe to split,\n",
    "                target_column = the column containing the values to split\n",
    "                separator = the symbol used to perform the split\n",
    "                returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "                The values in the other columns are duplicated across the newly divided rows.\n",
    "                '''\n",
    "                def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "                    split_row = row[target_column].split(separator)\n",
    "                    for s in split_row:\n",
    "                        new_row = row.to_dict()\n",
    "                        new_row[target_column] = s\n",
    "                        row_accumulator.append(new_row)\n",
    "                new_rows = []\n",
    "                df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\n",
    "                new_df = pd.DataFrame(new_rows)\n",
    "                return new_df\n",
    "\n",
    "            # filter out duplicates if there are only three weeks of data in the df\n",
    "            if not fixed_df['week_num'].isin([\"4\"]).any() and not fixed_df['week_num'].isin([\"5\"]).any() and not fixed_df['week_num'].isin([\"6\"]).any():\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'monday', ' ')\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'tuesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'wednesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'thursday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'friday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "\n",
    "            # filter out duplicates if there are four weeks of data in the df\n",
    "            elif not fixed_df['week_num'].isin([\"5\"]).any() and not fixed_df['week_num'].isin([\"6\"]).any():\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'monday', ' ')\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'tuesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'wednesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'thursday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'friday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "\n",
    "            # filter out duplicates if there are five weeks of data in the df\n",
    "            elif not fixed_df['week_num'].isin([\"6\"]).any():\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'monday', ' ')\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'tuesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'wednesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'thursday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'friday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "\n",
    "            # filter out duplicates if there are six weeks of data in the df\n",
    "            else:\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'monday', ' ')\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'tuesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'wednesday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'thursday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "                fixed_df = splitDataFrameList(fixed_df, 'friday', ' ')\n",
    "                fixed_df = fixed_df.iloc[[0, 1, 2, 5, 6, 9, 10, 13, 14, 17, 18, 21]]\n",
    "                fixed_df = fixed_df.reset_index(drop=True)\n",
    "\n",
    "            # Add new 'Entree' column\n",
    "            fixed_df['entree'] = np.where(fixed_df.index % 2 == 0, '1', '2')\n",
    "\n",
    "            # Add new 'Year' column\n",
    "            fixed_df['year'] = year\n",
    "\n",
    "            # Add new 'School' column\n",
    "            fixed_df['school'] = school\n",
    "            \n",
    "            # Add new 'Pre_Post' column\n",
    "            fixed_df['pre_post'] = pre_post\n",
    "\n",
    "            # Add new 'uuid' column\n",
    "            fixed_df['uuid'] = [uuid.uuid4() for i in range(len(fixed_df))]\n",
    "\n",
    "############## END OF ADDING COLUMNS ###################\n",
    "\n",
    "\n",
    "            csv_string = fixed_df.to_csv(index=False)\n",
    "            # create the menu.csv key with s3ObjectKey\n",
    "            menu_key = f\"{s3_object_key}_menu.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=menu_key)\n",
    "            step_state[\"menuCsvKey\"] = menu_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "    #################### Write DataFrame to \"food.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains('Entrees').any():\n",
    "            # Strip extra spaces from column names and cells\n",
    "            df = df.applymap(str.strip)\n",
    "            df.columns = df.columns.astype(str).str.strip()\n",
    "\n",
    "            # Set column names to first row's values\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "\n",
    "            # rename columns\n",
    "            new_columns = ['entree', 'veg', 'fruit', 'grain']\n",
    "            df.columns = new_columns[:len(df.columns)]\n",
    "\n",
    "            # Add new 'Year' column\n",
    "            df['year'] = year\n",
    "\n",
    "            # Add new 'School' column\n",
    "            df['school'] = school\n",
    "                \n",
    "            # Add new 'Pre_Post' column\n",
    "            df['pre_post'] = pre_post\n",
    "\n",
    "            # Add new 'uuid' column\n",
    "            df['uuid'] = [uuid.uuid4() for i in range(len(df))]\n",
    "\n",
    "            csv_string = df.to_csv(index=False)\n",
    "            # create the food.csv key with s3ObjectKey\n",
    "            food_key = f\"{s3_object_key}_food.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=food_key)\n",
    "            step_state[\"foodCsvKey\"] = food_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "\n",
    "    ################# Write DataFrame to \"choices.csv\" in validation bucket\n",
    "    for df_name, df in df_dict.items():\n",
    "        if df[0].str.contains(\"# of entrée choices daily\").any():\n",
    "            # Strip extra spaces from column names and cells\n",
    "            df = df.applymap(str.strip)\n",
    "            df.columns = df.columns.astype(str).str.strip()\n",
    "            \n",
    "            # Set column names to first row's values\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:]\n",
    "\n",
    "            # renaming columns:\n",
    "            new_columns = [\"num_entree\", \"num_veg\", \"num_fruit\"]\n",
    "            df.columns = new_columns[:len(df.columns)]\n",
    "\n",
    "            # Add new 'Year' column\n",
    "            df['year'] = year\n",
    "\n",
    "            # Add new 'School' column\n",
    "            df['school'] = school\n",
    "\n",
    "            # Add new 'Pre_Post' column\n",
    "            df['pre_post'] = pre_post\n",
    "\n",
    "            # Add new 'uuid' column\n",
    "            df['uuid'] = [uuid.uuid4() for i in range(len(df))]\n",
    "\n",
    "            csv_string = df.to_csv(index=False)\n",
    "            # create the choices.csv key with s3ObjectKey\n",
    "            choices_key = f\"{s3_object_key}_choices.csv\"\n",
    "            s3_client.put_object(Body=csv_string, Bucket=validation_bucket, Key=choices_key)\n",
    "            step_state[\"choicesCsvKey\"] = choices_key\n",
    "            print(csv_string)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # Add validation bucket to step_state\n",
    "    step_state[\"validationBucket\"] = validation_bucket\n",
    "\n",
    "    print(json.dumps(step_state, default=str))\n",
    "    # This returns the updated step_state back to the state machine.\n",
    "    return step_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### \"Validate table\" step in step function #####\n",
    "####### THIS IS table_validator LAMBDA #####\n",
    "\n",
    "\n",
    "##### (not inserted in lambda function yet) #####\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_state = event['Input']['Payload']\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "    validation_bucket = step_state['validationBucket']\n",
    "    validation_key = 'validation.csv'  \n",
    "\n",
    "\n",
    "    try:\n",
    "        # Try to load existing validation data from S3\n",
    "        validation_object = s3_client.get_object(Bucket=validation_bucket, Key=validation_key)\n",
    "        validation_df = pd.read_csv(validation_object['Body'])\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        # If validation.csv does not exist yet, create an empty DataFrame\n",
    "        validation_df = pd.DataFrame(columns=[\"Timestamp\", \"MenuIsValid\", \"FoodIsValid\", \"ChoicesIsValid\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########## MENU validation ###############\n",
    "\n",
    "\n",
    "    try:\n",
    "        menu_csv_key = step_state['menuCsvKey']\n",
    "        menu_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=menu_csv_key)\n",
    "        menu_df = pd.read_csv(menu_csv_object['Body'])\n",
    "\n",
    "        if menu_df.shape[0] >= 1 and menu_df.shape[1] == 11:\n",
    "            step_state[\"menuIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"menuIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"menuIsValid\"] = False\n",
    "        print(\"MenuCsvKeyError\")\n",
    "\n",
    "    \n",
    "    ########## FOOD validation ###############\n",
    "\n",
    "    try:\n",
    "        food_csv_key = step_state['foodCsvKey']\n",
    "        food_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=food_csv_key)\n",
    "        food_df = pd.read_csv(food_csv_object['Body'])\n",
    "\n",
    "        if food_df.shape[0] >= 1 and food_df.shape[1] == 8 and food_df.iloc[0].astype(str).str.contains(r'[a-zA-Z0-9]').any():\n",
    "            step_state[\"foodIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"foodIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"foodIsValid\"] = False\n",
    "        print(\"FoodCsvKeyError\")\n",
    "\n",
    "\n",
    "\n",
    "    ########## CHOICES validation ###############\n",
    "\n",
    "    try:\n",
    "        choices_csv_key = step_state['choicesCsvKey']\n",
    "        choices_csv_object = s3_client.get_object(Bucket=validation_bucket, Key=choices_csv_key)\n",
    "        choices_df = pd.read_csv(choices_csv_object['Body'])\n",
    "\n",
    "        if choices_df.shape[0] == 1 and choices_df.shape[1] == 7:\n",
    "            step_state[\"choicesIsValid\"] = True\n",
    "        else:\n",
    "            step_state[\"choicesIsValid\"] = False\n",
    "\n",
    "    except KeyError:\n",
    "        step_state[\"choicesIsValid\"] = False\n",
    "        print(\"ChoicesCsvKeyError\")\n",
    "\n",
    "    # Add new validation results\n",
    "    new_results = {\n",
    "        \"Timestamp\": datetime.datetime.now(),\n",
    "        \"MenuIsValid\": step_state[\"menuIsValid\"],\n",
    "        \"FoodIsValid\": step_state[\"foodIsValid\"],\n",
    "        \"ChoicesIsValid\": step_state[\"choicesIsValid\"]\n",
    "    }\n",
    "\n",
    "    new_results_df = pd.DataFrame([new_results])\n",
    "\n",
    "    validation_df = pd.concat([validation_df, new_results_df])\n",
    "\n",
    "    # Save the updated validation_df back to S3\n",
    "    csv_buffer = StringIO()\n",
    "    validation_df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=validation_bucket, Key=validation_key)\n",
    "\n",
    "    # put CSV keys in step_state\n",
    "    step_state[\"validationCsvKey\"] = validation_key\n",
    "    step_state[\"menuCsvKey\"] = menu_csv_key\n",
    "    step_state[\"foodCsvKey\"] = food_csv_key\n",
    "    step_state[\"choicesCsvKey\"] = choices_csv_key\n",
    "    \n",
    "\n",
    "\n",
    "    return step_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT-TO-PROD LAMBDA #########\n",
    "#######  This is INSERT TO PROD Step function step ######\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    step_state = event['Input']['Payload']\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Retrieve input from the state machine\n",
    "\n",
    "    validation_bucket = step_state['validationBucket']\n",
    "    choices_csv_key = step_state['choicesCsvKey']\n",
    "    food_csv_key = step_state['foodCsvKey']\n",
    "    menu_csv_key = step_state['menuCsvKey']\n",
    "    validation_key = step_state['validationCsvKey']\n",
    "\n",
    "    step_state['productionBucket'] = 'prod-bucket---------doc-parser'\n",
    "    production_bucket = step_state['productionBucket']\n",
    "\n",
    "\n",
    "\n",
    "######## APPEND TO PRODUCTION BUCKET or CREATE IF DOESN'T EXIST #########\n",
    "\n",
    "    ########## MENU append ###############\n",
    "\n",
    "    if step_state[\"menuIsValid\"] == True:\n",
    "\n",
    "        try:\n",
    "            # Try to load existing production data from S3\n",
    "            prod_menu_object = s3_client.get_object(Bucket=production_bucket, Key=\"prod_menu_table.csv\")\n",
    "            prod_menu_df = pd.read_csv(prod_menu_object['Body'])\n",
    "        except s3_client.exceptions.NoSuchKey:\n",
    "            # If production.csv does not exist yet, create an empty DataFrame\n",
    "            prod_menu_df = pd.DataFrame(columns=[\"week_num\", \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"entree\", \"year\", \"school\", \"pre_post\", \"uuid\"])\n",
    "\n",
    "        menu_df = pd.read_csv(s3_client.get_object(Bucket=validation_bucket, Key=menu_csv_key)['Body'])\n",
    "        prod_menu_df = pd.concat([prod_menu_df, menu_df])\n",
    "\n",
    "        # reset the index, dropping the old one (bc concat keeps the old index)\n",
    "        prod_menu_df = prod_menu_df.reset_index(drop=True)\n",
    "\n",
    "        # Save the updated prod_menu_df back to S3\n",
    "        csv_buffer = StringIO()\n",
    "        prod_menu_df.to_csv(csv_buffer, index=False)\n",
    "        s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=production_bucket, Key=\"prod_menu_table.csv\")\n",
    "\n",
    "        # Save the updated prod_menu_df back to S3\n",
    "        csv_buffer = StringIO()\n",
    "        prod_menu_df.to_csv(csv_buffer, index=False)\n",
    "        response = s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=production_bucket, Key=\"prod_menu_table.csv\")\n",
    "\n",
    "        # if successful, delete menu_csv_key from validation bucket\n",
    "        if \"ETag\" in response:\n",
    "            s3_client.delete_object(Bucket=validation_bucket, Key=menu_csv_key)\n",
    "        else:\n",
    "            print(\"Error while saving to production bucket. Did not delete from validation bucket.\")\n",
    "\n",
    "\n",
    "\n",
    "    ########## FOOD append ###############\n",
    "\n",
    "    if step_state[\"foodIsValid\"] == True:\n",
    "\n",
    "        try:\n",
    "            # Try to load existing production data from S3\n",
    "            prod_food_object = s3_client.get_object(Bucket=production_bucket, Key=\"prod_food_table.csv\")\n",
    "            prod_food_df = pd.read_csv(prod_food_object['Body'])\n",
    "        except s3_client.exceptions.NoSuchKey:\n",
    "            # If production.csv does not exist yet, create an empty DataFrame\n",
    "            prod_food_df = pd.DataFrame(columns=[\"entree\", \"veg\", \"fruit\", \"grain\", \"year\", \"school\", \"pre_post\", \"uuid\"])\n",
    "\n",
    "        food_df = pd.read_csv(s3_client.get_object(Bucket=validation_bucket, Key=food_csv_key)['Body'])\n",
    "        prod_food_df = pd.concat([prod_food_df, food_df])\n",
    "\n",
    "        # reset the index, dropping the old one (bc concat keeps the old index)\n",
    "        prod_food_df = prod_food_df.reset_index(drop=True)\n",
    "\n",
    "        # Save the updated prod_food_df back to S3\n",
    "        csv_buffer = StringIO()\n",
    "        prod_food_df.to_csv(csv_buffer, index=False)\n",
    "        response = s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=production_bucket, Key=\"prod_food_table.csv\")\n",
    "\n",
    "        # if successful, delete food_csv_key from validation bucket\n",
    "        if \"ETag\" in response:\n",
    "            s3_client.delete_object(Bucket=validation_bucket, Key=food_csv_key)\n",
    "        else:\n",
    "            print(\"Error while saving to production bucket. Did not delete from validation bucket.\")\n",
    "\n",
    "\n",
    "\n",
    "    ########## CHOICES append ###############\n",
    "    if step_state[\"choicesIsValid\"] == True:\n",
    "\n",
    "        try:\n",
    "            #try to load existing production data from S3\n",
    "            prod_choices_object = s3_client.get_object(Bucket=production_bucket, Key=\"prod_choices_table.csv\")\n",
    "            prod_choices_df = pd.read_csv(prod_choices_object['Body'])\n",
    "        except s3_client.exceptions.NoSuchKey:\n",
    "            # If production.csv does not exist yet, create an empty DataFrame\n",
    "            prod_choices_df = pd.DataFrame(columns=[\"num_entree\", \"num_veg\", \"num_fruit\", \"year\", \"school\", \"pre_post\", \"uuid\"])\n",
    "\n",
    "        choices_df = pd.read_csv(s3_client.get_object(Bucket=validation_bucket, Key=choices_csv_key)['Body'])\n",
    "        prod_choices_df = pd.concat([prod_choices_df, choices_df])\n",
    "\n",
    "        # reset the index, dropping the old one (bc concat keeps the old index)\n",
    "        prod_choices_df = prod_choices_df.reset_index(drop=True)\n",
    "\n",
    "        # Save the updated prod_choices_df back to S3\n",
    "        csv_buffer = StringIO()\n",
    "        prod_choices_df.to_csv(csv_buffer, index=False)\n",
    "        response = s3_client.put_object(Body=csv_buffer.getvalue(), Bucket=production_bucket, Key=\"prod_choices_table.csv\")\n",
    "\n",
    "        # if successful, delete choices_csv_key from validation bucket\n",
    "        if \"ETag\" in response:\n",
    "            s3_client.delete_object(Bucket=validation_bucket, Key=choices_csv_key)\n",
    "        else:\n",
    "            print(\"Error while saving to production bucket. Did not delete from validation bucket.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
